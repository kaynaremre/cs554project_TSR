{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms, models\n",
    "from math import sqrt\n",
    "import itertools\n",
    "from torchvision.ops import RoIPool\n",
    "from torchvision.ops import nms\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "import imageio\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Box:\n",
    "  class_index: int\n",
    "  class_name: str\n",
    "  corners: np.ndarray\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return \"[class=%s (%f,%f,%f,%f)]\" % (self.class_name, self.corners[0], self.corners[1], self.corners[2], self.corners[3])\n",
    "\n",
    "  def __str__(self):\n",
    "    return repr(self)\n",
    "\n",
    "@dataclass\n",
    "class TrainingSample:\n",
    "  anchor_map:                 np.ndarray                # shape (feature_map_height,feature_map_width,num_anchors*4), with each anchor as [center_y,center_x,height,width]\n",
    "  anchor_valid_map:           np.ndarray                # shape (feature_map_height,feature_map_width,num_anchors), indicating which anchors are valid (do not cross image boundaries)\n",
    "  gt_rpn_map:                 np.ndarray                # TODO: describe me\n",
    "  gt_rpn_object_indices:      List[Tuple[int,int,int]]  # list of (y,x,k) coordinates of anchors in gt_rpn_map that are labeled as object\n",
    "  gt_rpn_background_indices:  List[Tuple[int,int,int]]  # list of (y,x,k) coordinates of background anchors\n",
    "  gt_boxes:                   List[Box]                 # list of ground-truth boxes, scaled\n",
    "  image_data:                 np.ndarray                # shape (3,height,width), pre-processed and scaled to size expected by model\n",
    "  image:                      Image                     # PIL image data (for debug rendering), scaled\n",
    "  filepath:                   str                       # file path of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_scale_factor(original_width, original_height, min_dimension_pixels):\n",
    "  if not min_dimension_pixels:\n",
    "    return 1.0\n",
    "  if original_width > original_height:\n",
    "    scale_factor = min_dimension_pixels / original_height\n",
    "  else:\n",
    "    scale_factor = min_dimension_pixels / original_width\n",
    "  return scale_factor\n",
    "\n",
    "def _preprocess_vgg16(image_data):\n",
    "  image_data = image_data[:, :, ::-1]           # RGB -> BGR\n",
    "  image_data[:, :, 0] -= 103.939                # ImageNet B mean\n",
    "  image_data[:, :, 1] -= 116.779                # ImageNet G mean\n",
    "  image_data[:, :, 2] -= 123.680                # ImageNet R mean \n",
    "  image_data = image_data.transpose([2, 0, 1])  # (height,width,3) -> (3,height,width)\n",
    "  return image_data.copy()                      # copy required to eliminate negative stride (which Torch doesn't like)\n",
    "\n",
    "def load_image(url, min_dimension_pixels = None, horizontal_flip = False):\n",
    "  \"\"\"\n",
    "  Loads and preprocesses an image for use with VGG-16, which consists of\n",
    "  converting RGB to BGR and subtracting ImageNet dataset means from each\n",
    "  component. The image can be resized so that the minimum dimension is a\n",
    "  defined size, as recommended by Faster R-CNN. \n",
    "  Parameters\n",
    "  ----------\n",
    "  url : str\n",
    "    URL (local or remote file) to load.\n",
    "  min_dimension_pixels : int\n",
    "    If not None, specifies the size in pixels of the smaller side of the image.\n",
    "    The other side is scaled proportionally.\n",
    "  horizontal_flip : bool\n",
    "    Whether to flip the image horizontally.\n",
    "  Returns\n",
    "  -------\n",
    "  np.ndarray, PIL.Image, float, Tuple[int, int, int]\n",
    "    Image pixels as float32, shaped as (channels, height, width); an image\n",
    "    object suitable for drawing and visualization; scaling factor applied to\n",
    "    the image dimensions; and the original image shape.\n",
    "  \"\"\"\n",
    "  data = imageio.imread(url, pilmode = \"RGB\")\n",
    "  image = Image.fromarray(data, mode = \"RGB\")\n",
    "  original_width, original_height = image.width, image.height\n",
    "  if horizontal_flip:\n",
    "    image = image.transpose(method = Image.FLIP_LEFT_RIGHT)\n",
    "  if min_dimension_pixels is not None:\n",
    "    scale_factor = _compute_scale_factor(original_width = image.width, original_height = image.height, min_dimension_pixels = min_dimension_pixels)\n",
    "    width = int(image.width * scale_factor)\n",
    "    height = int(image.height * scale_factor)\n",
    "    image = image.resize((width, height), resample = Image.BILINEAR)\n",
    "  else:\n",
    "    scale_factor = 1.0\n",
    "  image_data = np.array(image).astype(np.float32)\n",
    "  image_data = _preprocess_vgg16(image_data = image_data)\n",
    "  return image_data, image, scale_factor, (image_data.shape[0], original_height, original_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "  \"\"\"\n",
    "  A VOC dataset iterator for a particular split (train, val, etc.)\n",
    "  \"\"\"\n",
    "\n",
    "  num_classes = 10\n",
    "  class_index_to_name = {\n",
    "    0:  \"background\",\n",
    "    1:  \"keepRight\",\n",
    "    2:  \"merge\",\n",
    "    3:  \"pedestrianCrossing\",\n",
    "    4:  \"signalAhead\",\n",
    "    5:  \"speedLimit25\",\n",
    "    6:  \"speedLimit35\",\n",
    "    7:  \"stop\",\n",
    "    8:  \"yield\",\n",
    "    9:  \"yieldAhead\",\n",
    "  }\n",
    "\n",
    "  def __init__(self, split, dir = \"lisa_resize/db_lisa_tiny\", feature_pixels = 16, augment = True, shuffle = True, allow_difficult = False, cache = True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    split : str\n",
    "      Dataset split to load: train, val, or trainval.\n",
    "    dir : str\n",
    "      Root directory of dataset.\n",
    "    feature_pixels : int\n",
    "      Size of each cell in the Faster R-CNN feature map (i.e., VGG-16 feature\n",
    "      extractor output) in image pixels. This is the separation distance\n",
    "      between anchors.\n",
    "    augment : bool\n",
    "      Whether to randomly augment (horizontally flip) images during iteration\n",
    "      with 50% probability.\n",
    "    shuffle : bool\n",
    "      Whether to shuffle the dataset each time it is iterated.\n",
    "    allow_difficult : bool\n",
    "      Whether to include ground truth boxes that are marked as \"difficult\".\n",
    "    cache : bool\n",
    "      Whether to training samples in memory after first being generated.\n",
    "    \"\"\"\n",
    "    # if not os.path.exists(dir):\n",
    "    #   raise FileNotFoundError(\"Dataset directory does not exist: %s\" % dir)\n",
    "    self.split = split\n",
    "    #self._dir = dir\n",
    "    self.class_index_to_name = self._get_classes()\n",
    "    self.class_name_to_index = { class_name: class_index for (class_index, class_name) in self.class_index_to_name.items() }\n",
    "    self.num_classes = len(self.class_index_to_name)\n",
    "    assert self.num_classes == Dataset.num_classes, \"Dataset does not have the expected number of classes (found %d but expected %d)\" % (self.num_classes, Dataset.num_classes)\n",
    "    assert self.class_index_to_name == Dataset.class_index_to_name, \"Dataset does not have the expected class mapping\"\n",
    "    self._filepaths = self._get_filepaths()\n",
    "    self.num_samples = len(self._filepaths)\n",
    "    self._gt_boxes_by_filepath = self._get_ground_truth_boxes(filepaths = self._filepaths, allow_difficult = allow_difficult) \n",
    "    self._i = 0\n",
    "    self._iterable_filepaths = self._filepaths.copy()\n",
    "    self._feature_pixels = feature_pixels\n",
    "    self._augment = augment\n",
    "    self._shuffle = shuffle\n",
    "    self._cache = cache\n",
    "    self._unaugmented_cached_sample_by_filepath = {}\n",
    "    self._augmented_cached_sample_by_filepath = {}\n",
    "\n",
    "  def __iter__(self):\n",
    "    self._i = 0\n",
    "    if self._shuffle:\n",
    "      random.shuffle(self._iterable_filepaths)\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    if self._i >= len(self._iterable_filepaths):\n",
    "      raise StopIteration\n",
    "\n",
    "    # Next file to load\n",
    "    filepath = self._iterable_filepaths[self._i]\n",
    "    self._i += 1\n",
    "\n",
    "    # Augment?\n",
    "    flip = random.randint(0, 1) != 0 if self._augment else 0\n",
    "    cached_sample_by_filepath = self._augmented_cached_sample_by_filepath if flip else self._unaugmented_cached_sample_by_filepath\n",
    "  \n",
    "    # Load and, if caching, write back to cache\n",
    "    if filepath in cached_sample_by_filepath:\n",
    "      sample = cached_sample_by_filepath[filepath]\n",
    "    else:\n",
    "      sample = self._generate_training_sample(filepath = filepath, flip = flip)\n",
    "    if self._cache:\n",
    "      cached_sample_by_filepath[filepath] = sample\n",
    "\n",
    "    # Return the sample\n",
    "    return sample\n",
    "\n",
    "  def _generate_training_sample(self, filepath, flip):\n",
    "    # Load and preprocess the image\n",
    "    scaled_image_data, scaled_image, scale_factor, original_shape = load_image(url = filepath, min_dimension_pixels = None, horizontal_flip = flip)\n",
    "    _, original_height, original_width = original_shape\n",
    "\n",
    "    # Scale ground truth boxes to new image size\n",
    "    scaled_gt_boxes = []\n",
    "    for box in self._gt_boxes_by_filepath[filepath]:\n",
    "      if flip:\n",
    "        corners = np.array([\n",
    "          box.corners[0],\n",
    "          original_width - 1 - box.corners[3],\n",
    "          box.corners[2],\n",
    "          original_width - 1 - box.corners[1]\n",
    "        ]) \n",
    "      else:\n",
    "        corners = box.corners\n",
    "      scaled_box = Box(\n",
    "        class_index = box.class_index,\n",
    "        class_name = box.class_name,\n",
    "        corners = corners * scale_factor \n",
    "      )\n",
    "      scaled_gt_boxes.append(scaled_box)\n",
    "\n",
    "    # Generate anchor maps and RPN truth map\n",
    "    anchor_map, anchor_valid_map = generate_anchor_maps(image_shape = scaled_image_data.shape, feature_pixels = self._feature_pixels)\n",
    "    gt_rpn_map, gt_rpn_object_indices, gt_rpn_background_indices = generate_rpn_map(anchor_map = anchor_map, anchor_valid_map = anchor_valid_map, gt_boxes = scaled_gt_boxes)\n",
    "\n",
    "    # Return sample\n",
    "    return TrainingSample(\n",
    "      anchor_map = anchor_map,\n",
    "      anchor_valid_map = anchor_valid_map,\n",
    "      gt_rpn_map = gt_rpn_map,\n",
    "      gt_rpn_object_indices = gt_rpn_object_indices,\n",
    "      gt_rpn_background_indices = gt_rpn_background_indices,\n",
    "      gt_boxes = scaled_gt_boxes,\n",
    "      image_data = scaled_image_data,\n",
    "      image = scaled_image,\n",
    "      filepath = filepath\n",
    "    )\n",
    "\n",
    "  def _get_classes(self):\n",
    "    imageset_dir = \"annotations_FasterRCNN.csv\"\n",
    "    anno = pd.read_csv(imageset_dir)\n",
    "    classes = set([ anno[\"class\"] ])\n",
    "    assert len(classes) > 0, \"No classes found in ImageSets/Main\"\n",
    "    class_index_to_name = { (1 + v[0]): v[1] for v in enumerate(sorted(classes)) }\n",
    "    class_index_to_name[0] = \"background\"\n",
    "    return class_index_to_name\n",
    "\n",
    "  def _get_filepaths(self):\n",
    "    imageset_dir = \"annotations_FasterRCNN.csv\"\n",
    "    anno = pd.read_csv(imageset_dir)\n",
    "    image_paths = []\n",
    "    for index, row in anno.iterrows():\n",
    "      if row[\"Train/Test\"] == self.split:\n",
    "        image_paths.append(row[\"filename\"])\n",
    "        \n",
    "    return image_paths\n",
    "\n",
    "\n",
    "\n",
    "  def _get_ground_truth_boxes(self, filepaths, allow_difficult):\n",
    "    gt_boxes_by_filepath = {}\n",
    "    imageset_dir = \"annotations_FasterRCNN.csv\"\n",
    "    anno = pd.read_csv(imageset_dir)\n",
    "\n",
    "  \n",
    "\n",
    "    for index, row in anno.iterrows():\n",
    "      if row[\"Train/Test\"] == self.split:\n",
    "        boxes = []\n",
    "        x_min = row[\"x1\"]      \n",
    "        y_min = row[\"y1\"]\n",
    "        x_max = row[\"x2\"]\n",
    "        y_max = row[\"y2\"]\n",
    "\n",
    "        class_name = row[\"class\"]\n",
    "\n",
    "        corners = np.array([ y_min, x_min, y_max, x_max ]).astype(np.float32)\n",
    "        box = Box(class_index = self.class_name_to_index[class_name], class_name = class_name, corners = corners)\n",
    "        boxes.append(box)\n",
    "        assert len(boxes) > 0\n",
    "        gt_boxes_by_filepath[row[\"filename\"]] = boxes\n",
    "    return gt_boxes_by_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes1, boxes2):\n",
    "  \"\"\"\n",
    "  Computes intersection-over-union (IoU) for multiple boxes in parallel.\n",
    "  Parameters\n",
    "  ----------\n",
    "  boxes1 : np.ndarray\n",
    "    Box corners, shaped (N, 4), with each box as (y1, x1, y2, x2).\n",
    "  boxes2 : np.ndarray\n",
    "    Box corners, shaped (M, 4).\n",
    "  Returns\n",
    "  -------\n",
    "  np.ndarray\n",
    "    IoUs for each pair of boxes in boxes1 and boxes2, shaped (N, M).\n",
    "  \"\"\"\n",
    "  top_left_point = np.maximum(boxes1[:,None,0:2], boxes2[:,0:2])                                  # (N,1,2) and (M,2) -> (N,M,2) indicating top-left corners of box pairs\n",
    "  bottom_right_point = np.minimum(boxes1[:,None,2:4], boxes2[:,2:4])                              # \"\" bottom-right corners \"\"\n",
    "  well_ordered_mask = np.all(top_left_point < bottom_right_point, axis = 2)                       # (N,M) indicating whether top_left_x < bottom_right_x and top_left_y < bottom_right_y (meaning boxes may intersect)\n",
    "  intersection_areas = well_ordered_mask * np.prod(bottom_right_point - top_left_point, axis = 2) # (N,M) indicating intersection area (bottom_right_x - top_left_x) * (bottom_right_y - top_left_y)\n",
    "  areas1 = np.prod(boxes1[:,2:4] - boxes1[:,0:2], axis = 1)                                       # (N,) indicating areas of boxes1\n",
    "  areas2 = np.prod(boxes2[:,2:4] - boxes2[:,0:2], axis = 1)                                       # (M,) indicating areas of boxes2\n",
    "  union_areas = areas1[:,None] + areas2 - intersection_areas                                      # (N,1) + (M,) - (N,M) = (N,M), union areas of both boxes\n",
    "  epsilon = 1e-7\n",
    "  return intersection_areas / (union_areas + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_intersection_over_union(boxes1, boxes2):\n",
    "  \"\"\"\n",
    "  Equivalent to intersection_over_union(), operating on PyTorch tensors.\n",
    "  Parameters\n",
    "  ----------\n",
    "  boxes1 : torch.Tensor\n",
    "    Box corners, shaped (N, 4), with each box as (y1, x1, y2, x2).\n",
    "  boxes2 : torch.Tensor\n",
    "    Box corners, shaped (M, 4).\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor\n",
    "    IoUs for each pair of boxes in boxes1 and boxes2, shaped (N, M).\n",
    "  \"\"\"\n",
    "  top_left_point = t.maximum(boxes1[:,None,0:2], boxes2[:,0:2])                                 # (N,1,2) and (M,2) -> (N,M,2) indicating top-left corners of box pairs\n",
    "  bottom_right_point = t.minimum(boxes1[:,None,2:4], boxes2[:,2:4])                             # \"\" bottom-right corners \"\"\n",
    "  well_ordered_mask = t.all(top_left_point < bottom_right_point, axis = 2)                      # (N,M) indicating whether top_left_x < bottom_right_x and top_left_y < bottom_right_y (meaning boxes may intersect)\n",
    "  intersection_areas = well_ordered_mask * t.prod(bottom_right_point - top_left_point, dim = 2) # (N,M) indicating intersection area (bottom_right_x - top_left_x) * (bottom_right_y - top_left_y)\n",
    "  areas1 = t.prod(boxes1[:,2:4] - boxes1[:,0:2], dim = 1)                                       # (N,) indicating areas of boxes1\n",
    "  areas2 = t.prod(boxes2[:,2:4] - boxes2[:,0:2], dim = 1)                                       # (M,) indicating areas of boxes2\n",
    "  union_areas = areas1[:,None] + areas2 - intersection_areas                                    # (N,1) + (M,) - (N,M) = (N,M), union areas of both boxes\n",
    "  epsilon = 1e-7\n",
    "  return intersection_areas / (union_areas + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_deltas_to_boxes(box_deltas, anchors, box_delta_means, box_delta_stds):\n",
    "  \"\"\"\n",
    "  Converts box deltas, which are in parameterized form (ty, tx, th, tw) as\n",
    "  described by the Fast R-CNN and Faster R-CNN papers, to boxes\n",
    "  (y1, x1, y2, x2). The anchors are the base boxes (e.g., RPN anchors or\n",
    "  proposals) that the box deltas describe a modification to.\n",
    "  Parameters\n",
    "  ----------\n",
    "  box_deltas : np.ndarray\n",
    "    Box deltas with shape (N, 4). Each row is (ty, tx, th, tw).\n",
    "  anchors : np.ndarray\n",
    "    Corresponding anchors that the box deltas are based upon, shaped (N, 4)\n",
    "    with each row being (center_y, center_x, height, width).\n",
    "  box_delta_means : np.ndarray\n",
    "    Mean ajustment to box deltas, (4,), to be added after standard deviation\n",
    "    scaling and before conversion to actual box coordinates.\n",
    "  box_delta_stds : np.ndarray\n",
    "    Standard deviation adjustment to box deltas, (4,). Box deltas are first\n",
    "    multiplied by these values.\n",
    "  Returns\n",
    "  -------\n",
    "  np.ndarray\n",
    "    Box coordinates, (N, 4), with each row being (y1, x1, y2, x2).\n",
    "  \"\"\"\n",
    "  box_deltas = box_deltas * box_delta_stds + box_delta_means\n",
    "  center = anchors[:,2:4] * box_deltas[:,0:2] + anchors[:,0:2]  # center_x = anchor_width * tx + anchor_center_x, center_y = anchor_height * ty + anchor_center_y\n",
    "  size = anchors[:,2:4] * np.exp(box_deltas[:,2:4])             # width = anchor_width * exp(tw), height = anchor_height * exp(th)\n",
    "  boxes = np.empty(box_deltas.shape)\n",
    "  boxes[:,0:2] = center - 0.5 * size                            # y1, x1\n",
    "  boxes[:,2:4] = center + 0.5 * size                            # y2, x2\n",
    "  return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_convert_deltas_to_boxes(box_deltas, anchors, box_delta_means, box_delta_stds):\n",
    "  \"\"\"\n",
    "  Equivalent to convert_deltas_to_boxes(), operating on PyTorch tensors.\n",
    "  Parameters\n",
    "  ----------\n",
    "  box_deltas : torch.Tensor\n",
    "    Box deltas with shape (N, 4). Each row is (ty, tx, th, tw).\n",
    "  anchors : torch.Tensor\n",
    "    Corresponding anchors that the box deltas are based upon, shaped (N, 4)\n",
    "    with each row being (center_y, center_x, height, width).\n",
    "  box_delta_means : torch.Tensor\n",
    "    Mean ajustment to box deltas, (4,), to be added after standard deviation\n",
    "    scaling and before conversion to actual box coordinates.\n",
    "  box_delta_stds : torch.Tensor\n",
    "    Standard deviation adjustment to box deltas, (4,). Box deltas are first\n",
    "    multiplied by these values.\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor\n",
    "    Box coordinates, (N, 4), with each row being (y1, x1, y2, x2).\n",
    "  \"\"\"\n",
    "  box_deltas = box_deltas * box_delta_stds + box_delta_means\n",
    "  center = anchors[:,2:4] * box_deltas[:,0:2] + anchors[:,0:2]  # center_x = anchor_width * tx + anchor_center_x, center_y = anchor_height * ty + anchor_center_y\n",
    "  size = anchors[:,2:4] * t.exp(box_deltas[:,2:4])              # width = anchor_width * exp(tw), height = anchor_height * exp(th)\n",
    "  boxes = t.empty(box_deltas.shape, dtype = t.float32, device = \"cuda\")\n",
    "  boxes[:,0:2] = center - 0.5 * size                            # y1, x1\n",
    "  boxes[:,2:4] = center + 0.5 * size                            # y2, x2\n",
    "  return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_anchor_sizes():\n",
    "  #\n",
    "  # Anchor scales and aspect ratios.\n",
    "  #\n",
    "  # x * y = area          x * (x_aspect * x) = x_aspect * x^2 = area\n",
    "  # x_aspect * x = y  ->  x = sqrt(area / x_aspect)\n",
    "  #                       y = x_aspect * sqrt(area / x_aspect)\n",
    "  #\n",
    "  areas = [ 64*64, 128*128, 256*256 ]   # pixels\n",
    "  x_aspects = [ 0.5, 1.0, 2.0 ]           # x:1 ratio\n",
    "\n",
    "  # Generate all 9 combinations of area and aspect ratio\n",
    "  heights = np.array([ round(x_aspects[j] * sqrt(areas[i] / x_aspects[j])) for (i, j) in itertools.product(range(3), range(3)) ])\n",
    "  widths = np.array([ round(sqrt(areas[i] / x_aspects[j])) for (i, j) in itertools.product(range(3), range(3)) ])\n",
    "\n",
    "  # Return as (9,2) matrix of sizes\n",
    "  return np.vstack([ heights, widths ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_maps(image_shape, feature_pixels): \n",
    "  \"\"\"\n",
    "  Generates maps defining the anchors for a given input image size. There are 9\n",
    "  different anchors at each feature map cell (3 scales, 3 ratios).\n",
    "  Parameters\n",
    "  ----------\n",
    "  image_shape : Tuple[int, int, int]\n",
    "    Shape of the input image, (channels, height, width), at the scale it will\n",
    "    be passed into the Faster R-CNN model.\n",
    "  feature_pixels : int\n",
    "    Distance in pixels between anchors. This is the size, in input image space,\n",
    "    of each cell of the feature map output by the feature extractor stage of\n",
    "    the Faster R-CNN network.\n",
    "  Returns\n",
    "  -------\n",
    "  np.ndarray, np.ndarray\n",
    "    Two maps, with height and width corresponding to the feature map\n",
    "    dimensions, not the input image:\n",
    "      1. A map of shape (height, width, num_anchors*4) containing all anchors,\n",
    "         each stored as (center_y, center_x, anchor_height, anchor_width) in\n",
    "         input image pixel space.\n",
    "      2. A map of shape (height, width, num_anchors) indicating which anchors\n",
    "         are valid (1) or invalid (0). Invalid anchors are those that cross\n",
    "         image boundaries and must not be used during training.\n",
    "  \"\"\"\n",
    "\n",
    "  assert len(image_shape) == 3\n",
    "\n",
    "  #\n",
    "  # Note that precision can strongly affect anchor labeling in some images.\n",
    "  # Conversion of both operands to float32 matches the implementation by Yun\n",
    "  # Chen. That is, changing the final line so as to eliminate the conversion to\n",
    "  # float32:\n",
    "  #\n",
    "  #   return anchor_map, anchor_valid_map\n",
    "  #\n",
    "  # Has a pronounced effect on positive anchors in image 2008_000028.jpg in\n",
    "  # VOC2012.\n",
    "  #\n",
    "  \n",
    "  # Base anchor template: (num_anchors,4), with each anchor being specified by\n",
    "  # its corners (y1,x1,y2,x2)\n",
    "  anchor_sizes = _compute_anchor_sizes()\n",
    "  num_anchors = anchor_sizes.shape[0]\n",
    "  anchor_template = np.empty((num_anchors, 4))\n",
    "  anchor_template[:,0:2] = -0.5 * anchor_sizes  # y1, x1 (top-left)\n",
    "  anchor_template[:,2:4] = +0.5 * anchor_sizes  # y2, x2 (bottom-right)\n",
    "\n",
    "  # Shape of map, (H,W), determined by VGG-16 backbone\n",
    "  height, width = image_shape[1] // feature_pixels, image_shape[2] // feature_pixels\n",
    "\n",
    "  # Generate (H,W,2) map of coordinates, in feature space, each being [y,x]\n",
    "  y_cell_coords = np.arange(height)\n",
    "  x_cell_coords = np.arange(width)\n",
    "  cell_coords = np.array(np.meshgrid(y_cell_coords, x_cell_coords)).transpose([2, 1, 0])\n",
    "\n",
    "  # Convert all coordinates to image space (pixels) at *center* of each cell\n",
    "  center_points = cell_coords * feature_pixels + 0.5 * feature_pixels\n",
    "\n",
    "  # (H,W,2) -> (H,W,4), repeating the last dimension so it contains (y,x,y,x)\n",
    "  center_points = np.tile(center_points, reps = 2)\n",
    "\n",
    "  # (H,W,4) -> (H,W,4*num_anchors)\n",
    "  center_points = np.tile(center_points, reps = num_anchors)\n",
    "  \n",
    "  #\n",
    "  # Now we can create the anchors by adding the anchor template to each cell\n",
    "  # location. Anchor template is flattened to size num_anchors * 4 to make \n",
    "  # the addition possible (along the last dimension). \n",
    "  #\n",
    "  anchors = center_points.astype(np.float32) + anchor_template.flatten()\n",
    "\n",
    "  # (H,W,4*num_anchors) -> (H*W*num_anchors,4)\n",
    "  anchors = anchors.reshape((height*width*num_anchors, 4))\n",
    "\n",
    "  # Valid anchors are those that do not cross image boundaries\n",
    "  image_height, image_width = image_shape[1:]\n",
    "  valid = np.all((anchors[:,0:2] >= [0,0]) & (anchors[:,2:4] <= [image_height,image_width]), axis = 1)\n",
    "\n",
    "  # Convert anchors to anchor format: (center_y, center_x, height, width)\n",
    "  anchor_map = np.empty((anchors.shape[0], 4))\n",
    "  anchor_map[:,0:2] = 0.5 * (anchors[:,0:2] + anchors[:,2:4])\n",
    "  anchor_map[:,2:4] = anchors[:,2:4] - anchors[:,0:2]\n",
    "\n",
    "  # Reshape maps and return\n",
    "  anchor_map = anchor_map.reshape((height, width, num_anchors * 4))\n",
    "  anchor_valid_map = valid.reshape((height, width, num_anchors))\n",
    "  return anchor_map.astype(np.float32), anchor_valid_map.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rpn_map(anchor_map, anchor_valid_map, gt_boxes, object_iou_threshold = 0.7, background_iou_threshold = 0.3):\n",
    "  \"\"\"\n",
    "  Generates a map containing ground truth data for training the region proposal\n",
    "  network.\n",
    "  Parameters\n",
    "  ----------\n",
    "  anchor_map : np.ndarray\n",
    "    Map of shape (height, width, num_anchors*4) defining the anchors as\n",
    "    (center_y, center_x, anchor_height, anchor_width) in input image space.\n",
    "  anchor_valid_map : np.ndarray\n",
    "    Map of shape (height, width, num_anchors) defining anchors that are valid\n",
    "    and may be included in training.\n",
    "  gt_boxes : List[training_sample.Box]\n",
    "    List of ground truth boxes.\n",
    "  object_iou_threshold : float\n",
    "    IoU threshold between an anchor and a ground truth box above which an\n",
    "    anchor is labeled as an object (positive) anchor.\n",
    "  background_iou_threshold : float\n",
    "    IoU threshold below which an anchor is labeled as background (negative).\n",
    "  Returns\n",
    "  -------\n",
    "  np.ndarray, np.ndarray, np.ndarray\n",
    "    RPN ground truth map, object (positive) anchor indices, and background\n",
    "    (negative) anchor indices. Map height and width dimensions are in feature\n",
    "    space.\n",
    "    1. RPN ground truth map of shape (height, width, num_anchors, 6) where the\n",
    "       last dimension is:\n",
    "       - 0: Trainable anchor (1) or not (0). Only valid and non-neutral (that\n",
    "            is, definitely positive or negative) anchors are trainable. This is\n",
    "            the same as anchor_valid_map with additional invalid anchors caused\n",
    "            by neutral samples\n",
    "       - 1: For trainable anchors, whether the anchor is an object anchor (1)\n",
    "            or background anchor (0). For non-trainable anchors, will be 0.\n",
    "       - 2: Regression target for box center, ty.\n",
    "       - 3: Regression target for box center, tx.\n",
    "       - 4: Regression target for box size, th.\n",
    "       - 5: Regression target for box size, tw.\n",
    "    2. Map of shape (N, 3) of indices (y, x, k) of all N object anchors in the\n",
    "       RPN ground truth map.\n",
    "    3. Map of shape (M, 3) of indices of all M background anchors in the RPN\n",
    "       ground truth map.\n",
    "  \"\"\"\n",
    "  height, width, num_anchors = anchor_valid_map.shape\n",
    "\n",
    "  # Convert ground truth box corners to (M,4) tensor and class indices to (M,)\n",
    "  gt_box_corners = np.array([ box.corners for box in gt_boxes ])\n",
    "  num_gt_boxes = len(gt_boxes)\n",
    "\n",
    "  # Compute ground truth box center points and side lengths\n",
    "  gt_box_centers = 0.5 * (gt_box_corners[:,0:2] + gt_box_corners[:,2:4])\n",
    "  gt_box_sides = gt_box_corners[:,2:4] - gt_box_corners[:,0:2]\n",
    "\n",
    "  # Flatten anchor boxes to (N,4) and convert to corners\n",
    "  anchor_map = anchor_map.reshape((-1,4))\n",
    "  anchors = np.empty(anchor_map.shape)\n",
    "  anchors[:,0:2] = anchor_map[:,0:2] - 0.5 * anchor_map[:,2:4]  # y1, x1\n",
    "  anchors[:,2:4] = anchor_map[:,0:2] + 0.5 * anchor_map[:,2:4]  # y2, x2\n",
    "  n = anchors.shape[0]\n",
    "\n",
    "  # Initialize all anchors initially as negative (background). We will also\n",
    "  # track which ground truth box was assigned to each anchor.\n",
    "  objectness_score = np.full(n, -1)   # RPN class: 0 = background, 1 = foreground, -1 = ignore (these will be marked as invalid in the truth map)\n",
    "  gt_box_assignments = np.full(n, -1) # -1 means no box\n",
    "  \n",
    "  # Compute IoU between each anchor and each ground truth box, (N,M).\n",
    "  ious = intersection_over_union(boxes1 = anchors, boxes2 = gt_box_corners)\n",
    "\n",
    "  # Need to remove anchors that are invalid (straddle image boundaries) from\n",
    "  # consideration entirely and the easiest way to do this is to wipe out their\n",
    "  # IoU scores\n",
    "  ious[anchor_valid_map.flatten() == 0, :] = -1.0\n",
    "\n",
    "  # Find the best IoU ground truth box for each anchor and the best IoU anchor\n",
    "  # for each ground truth box.\n",
    "  #\n",
    "  # Note that ious == max_iou_per_gt_box tests each of the N rows of ious\n",
    "  # against the M elements of max_iou_per_gt_box, column-wise. np.where() then\n",
    "  # returns all (y,x) indices of matches as a tuple: (y_indices, x_indices).\n",
    "  # The y indices correspond to the N dimension and therefore indicate anchors\n",
    "  # and the x indices correspond to the M dimension (ground truth boxes).\n",
    "  max_iou_per_anchor = np.max(ious, axis = 1)           # (N,)\n",
    "  best_box_idx_per_anchor = np.argmax(ious, axis = 1)   # (N,)\n",
    "  max_iou_per_gt_box = np.max(ious, axis = 0)           # (M,)\n",
    "  highest_iou_anchor_idxs = np.where(ious == max_iou_per_gt_box)[0] # get (L,) indices of anchors that are the highest-overlapping anchors for at least one of the M boxes\n",
    "\n",
    "  # Anchors below the minimum threshold are negative\n",
    "  objectness_score[max_iou_per_anchor < background_iou_threshold] = 0\n",
    "\n",
    "  # Anchors that meet the threshold IoU are positive\n",
    "  objectness_score[max_iou_per_anchor >= object_iou_threshold] = 1\n",
    "\n",
    "  # Anchors that overlap the most with ground truth boxes are positive\n",
    "  objectness_score[highest_iou_anchor_idxs] = 1\n",
    "\n",
    "  # We assign the highest IoU ground truth box to each anchor. If no box met\n",
    "  # the IoU threshold, the highest IoU box may happen to be a box for which\n",
    "  # the anchor had the highest IoU. If not, then the objectness score will be\n",
    "  # negative and the box regression won't ever be used.\n",
    "  gt_box_assignments[:] = best_box_idx_per_anchor\n",
    "\n",
    "  # Anchors that are to be ignored will be marked invalid. Generate a mask to\n",
    "  # multiply anchor_valid_map by (-1 -> 0, 0 or 1 -> 1). Then mark ignored\n",
    "  # anchors as 0 in objectness score because the score can only really be 0 or\n",
    "  # 1.\n",
    "  enable_mask = (objectness_score >= 0).astype(np.float32)\n",
    "  objectness_score[objectness_score < 0] = 0\n",
    "  \n",
    "  # Compute box delta regression targets for each anchor\n",
    "  box_delta_targets = np.empty((n, 4))\n",
    "  box_delta_targets[:,0:2] = (gt_box_centers[gt_box_assignments] - anchor_map[:,0:2]) / anchor_map[:,2:4] # ty = (box_center_y - anchor_center_y) / anchor_height, tx = (box_center_x - anchor_center_x) / anchor_width\n",
    "  box_delta_targets[:,2:4] = np.log(gt_box_sides[gt_box_assignments] / anchor_map[:,2:4])                 # th = log(box_height / anchor_height), tw = log(box_width / anchor_width)\n",
    "\n",
    "  # Assemble RPN ground truth map\n",
    "  rpn_map = np.zeros((height, width, num_anchors, 6))\n",
    "  rpn_map[:,:,:,0] = anchor_valid_map * enable_mask.reshape((height,width,num_anchors))  # trainable anchors (object or background; excludes boundary-crossing invalid and neutral anchors)\n",
    "  rpn_map[:,:,:,1] = objectness_score.reshape((height,width,num_anchors))\n",
    "  rpn_map[:,:,:,2:6] = box_delta_targets.reshape((height,width,num_anchors,4))\n",
    "  \n",
    "  # Return map along with positive and negative anchors\n",
    "  rpn_map_coords = np.transpose(np.mgrid[0:height,0:width,0:num_anchors], (1,2,3,0))                  # shape (height,width,k,3): every index (y,x,k,:) returns its own coordinate (y,x,k)\n",
    "  object_anchor_idxs = rpn_map_coords[np.where((rpn_map[:,:,:,1] > 0) & (rpn_map[:,:,:,0] > 0))]      # shape (N,3), where each row is the coordinate (y,x,k) of a positive sample\n",
    "  background_anchor_idxs = rpn_map_coords[np.where((rpn_map[:,:,:,1] == 0) & (rpn_map[:,:,:,0] > 0))] # shape (N,3), where each row is the coordinate (y,x,k) of a negative sample\n",
    "\n",
    "  return rpn_map.astype(np.float32), object_anchor_idxs, background_anchor_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorNetwork(nn.Module):\n",
    "  def __init__(self, num_classes, dropout_probability):\n",
    "    super().__init__()\n",
    "  \n",
    "    # Define network\n",
    "    self._roi_pool = RoIPool(output_size = (7, 7), spatial_scale = 1.0 / 16.0)\n",
    "    self._fc1 = nn.Linear(in_features = 512*7*7, out_features = 4096)\n",
    "    self._fc2 = nn.Linear(in_features = 4096, out_features = 4096)\n",
    "    self._classifier = nn.Linear(in_features = 4096, out_features = num_classes)\n",
    "    self._regressor = nn.Linear(in_features = 4096, out_features = (num_classes - 1) * 4) \n",
    "\n",
    "    # Dropout layers\n",
    "    self._dropout1 = nn.Dropout(p = dropout_probability)\n",
    "    self._dropout2 = nn.Dropout(p = dropout_probability)\n",
    "   \n",
    "    # Initialize weights\n",
    "    self._classifier.weight.data.normal_(mean = 0.0, std = 0.01)\n",
    "    self._classifier.bias.data.zero_()\n",
    "    self._regressor.weight.data.normal_(mean = 0.0, std = 0.001)\n",
    "    self._regressor.bias.data.zero_()\n",
    "\n",
    "  def forward(self, feature_map, proposals):\n",
    "    \"\"\"\n",
    "    Predict final class and box delta regressions for region-of-interest\n",
    "    proposals. The proposals serve as \"anchors\" for the box deltas, which\n",
    "    refine the proposals into final boxes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_map : torch.Tensor\n",
    "      Feature map of shape (batch_size, 512, height, width).\n",
    "    proposals : torch.Tensor\n",
    "      Region-of-interest box proposals that are likely to contain objects.\n",
    "      Has shape (N, 4), where N is the number of proposals, with each box given\n",
    "      as (y1, x1, y2, x2) in pixel coordinates.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor, torch.Tensor\n",
    "      Predicted classes, (N, num_classes), encoded as a one-hot vector, and\n",
    "      predicted box delta regressions, (N, 4*(num_classes-1)), where the deltas\n",
    "      are expressed as (ty, tx, th, tw) and are relative to each corresponding\n",
    "      proposal box. Because there is no box for the background class 0, it is\n",
    "      excluded entirely and only (num_classes-1) sets of box delta targets are\n",
    "      computed.\n",
    "    \"\"\"\n",
    "    # Batch size of one for now, so no need to associate proposals with batches\n",
    "    assert feature_map.shape[0] == 1, \"Batch size must be 1\"\n",
    "    batch_idxs = t.zeros((proposals.shape[0], 1)).cuda()\n",
    "\n",
    "    # (N, 5) tensor of (batch_idx, x1, y1, x2, y2)\n",
    "    indexed_proposals = t.cat([ batch_idxs, proposals ], dim = 1)\n",
    "    indexed_proposals = indexed_proposals[:, [ 0, 2, 1, 4, 3 ]] # each row, (batch_idx, y1, x1, y2, x2) -> (batch_idx, x1, y1, x2, y2)\n",
    "\n",
    "    # RoI pooling: (N, 512, 7, 7)\n",
    "    rois = self._roi_pool(feature_map, indexed_proposals)\n",
    "    rois = rois.reshape((rois.shape[0], 512*7*7)) # flatten each RoI: (N, 512*7*7)\n",
    "\n",
    "    # Forward propagate\n",
    "    y1o = F.relu(self._fc1(rois))\n",
    "    y1 = self._dropout1(y1o)\n",
    "    y2o = F.relu(self._fc2(y1))\n",
    "    y2 = self._dropout2(y2o)\n",
    "    classes_raw = self._classifier(y2)\n",
    "    classes = F.softmax(classes_raw, dim = 1)\n",
    "    box_deltas = self._regressor(y2)\n",
    "\n",
    "    return classes, box_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_loss(predicted_classes, y_true):\n",
    "  \"\"\"\n",
    "  Computes detector class loss. \n",
    "  Parameters\n",
    "  ----------\n",
    "  predicted_classes : torch.Tensor\n",
    "    RoI predicted classes as categorical vectors, (N, num_classes).\n",
    "  y_true : torch.Tensor\n",
    "    RoI class labels as categorical vectors, (N, num_classes).\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor\n",
    "    Scalar loss.\n",
    "  \"\"\"\n",
    "  epsilon = 1e-7\n",
    "  scale_factor = 1.0\n",
    "  cross_entropy_per_row = -(y_true * t.log(predicted_classes + epsilon)).sum(dim = 1)\n",
    "  N = cross_entropy_per_row.shape[0] + epsilon\n",
    "  cross_entropy = t.sum(cross_entropy_per_row) / N\n",
    "  return scale_factor * cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_loss(predicted_box_deltas, y_true):\n",
    "  \"\"\"\n",
    "  Computes detector regression loss.\n",
    "  Parameters\n",
    "  ----------\n",
    "  predicted_box_deltas : torch.Tensor\n",
    "    RoI predicted box delta regressions, (N, 4*(num_classes-1)). The background\n",
    "    class is excluded and only the non-background classes are included. Each\n",
    "    set of box deltas is stored in parameterized form as (ty, tx, th, tw).\n",
    "  y_true : torch.Tensor\n",
    "    RoI box delta regression ground truth labels, (N, 2, 4*(num_classes-1)).\n",
    "    These are stored as mask values (1 or 0) in (:,0,:) and regression\n",
    "    parameters in (:,1,:). Note that it is important to mask off the predicted\n",
    "    and ground truth values because they may be set to invalid values.\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor\n",
    "    Scalar loss.\n",
    "  \"\"\"\n",
    "  epsilon = 1e-7\n",
    "  scale_factor = 1.0\n",
    "  sigma = 1.0\n",
    "  sigma_squared = sigma * sigma\n",
    "\n",
    "  # We want to unpack the regression targets and the mask of valid targets into\n",
    "  # tensors each of the same shape as the predicted: \n",
    "  #   (num_proposals, 4*(num_classes-1))\n",
    "  # y_true has shape:\n",
    "  #   (num_proposals, 2, 4*(num_classes-1))\n",
    "  y_mask = y_true[:,0,:]\n",
    "  y_true_targets = y_true[:,1,:]\n",
    "\n",
    "  # Compute element-wise loss using robust L1 function for all 4 regression\n",
    "  # targets\n",
    "  x = y_true_targets - predicted_box_deltas\n",
    "  x_abs = t.abs(x)\n",
    "  is_negative_branch = (x < (1.0 / sigma_squared)).float()\n",
    "  R_negative_branch = 0.5 * x * x * sigma_squared\n",
    "  R_positive_branch = x_abs - 0.5 / sigma_squared\n",
    "  losses = is_negative_branch * R_negative_branch + (1.0 - is_negative_branch) * R_positive_branch\n",
    "\n",
    "  # Normalize to number of proposals (e.g., 128). Although this may not be\n",
    "  # what the paper does, it seems to work. Other implemetnations do this.\n",
    "  # Using e.g., the number of positive proposals will cause the loss to \n",
    "  # behave erratically because sometimes N will become very small.  \n",
    "  N = y_true.shape[0] + epsilon\n",
    "  relevant_loss_terms = y_mask * losses\n",
    "  return scale_factor * t.sum(relevant_loss_terms) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self._block1_conv1 = nn.Conv2d(in_channels = 3,  out_channels = 64, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block1_conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block1_pool = nn.MaxPool2d(kernel_size = (2, 2), stride = 2)\n",
    "\n",
    "    self._block2_conv1 = nn.Conv2d(in_channels = 64,  out_channels = 128, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block2_conv2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block2_pool = nn.MaxPool2d(kernel_size = (2, 2), stride = 2)\n",
    "\n",
    "    self._block3_conv1 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block3_conv2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block3_conv3 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block3_pool = nn.MaxPool2d(kernel_size = (2, 2), stride = 2)\n",
    "\n",
    "    self._block4_conv1 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block4_conv2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block4_conv3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block4_pool = nn.MaxPool2d(kernel_size = (2, 2), stride = 2)\n",
    "\n",
    "    self._block5_conv1 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block5_conv2 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._block5_conv3 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "\n",
    "    # Freeze first two convolutional blocks\n",
    "    self._block1_conv1.weight.requires_grad = False\n",
    "    self._block1_conv1.bias.requires_grad = False\n",
    "    self._block1_conv2.weight.requires_grad = False\n",
    "    self._block1_conv2.bias.requires_grad = False\n",
    "\n",
    "    self._block2_conv1.weight.requires_grad = False\n",
    "    self._block2_conv1.bias.requires_grad = False\n",
    "    self._block2_conv2.weight.requires_grad = False\n",
    "    self._block2_conv2.bias.requires_grad = False\n",
    "\n",
    "  def forward(self, image_data):\n",
    "    \"\"\"\n",
    "    Converts input images into feature maps using VGG-16 convolutional layers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_data : torch.Tensor\n",
    "      A tensor of shape (batch_size, channels, height, width) representing\n",
    "      images normalized using the VGG-16 convention (BGR, ImageNet channel-wise\n",
    "      mean-centered).\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "      Feature map of shape (batch_size, 512, height // 16, width // 16).\n",
    "    \"\"\"\n",
    "    y = F.relu(self._block1_conv1(image_data))\n",
    "    y = F.relu(self._block1_conv2(y))\n",
    "    y = self._block1_pool(y)\n",
    "\n",
    "    y = F.relu(self._block2_conv1(y))\n",
    "    y = F.relu(self._block2_conv2(y))\n",
    "    y = self._block2_pool(y)\n",
    "\n",
    "    y = F.relu(self._block3_conv1(y))\n",
    "    y = F.relu(self._block3_conv2(y))\n",
    "    y = F.relu(self._block3_conv3(y))\n",
    "    y = self._block3_pool(y)\n",
    "\n",
    "    y = F.relu(self._block4_conv1(y))\n",
    "    y = F.relu(self._block4_conv2(y))\n",
    "    y = F.relu(self._block4_conv3(y))\n",
    "    y = self._block4_pool(y)\n",
    "\n",
    "    y = F.relu(self._block5_conv1(y))\n",
    "    y = F.relu(self._block5_conv2(y))\n",
    "    y = F.relu(self._block5_conv3(y))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "  def __init__(self, allow_edge_proposals = False):\n",
    "    super().__init__()\n",
    "\n",
    "    # Constants\n",
    "    self._allow_edge_proposals = allow_edge_proposals\n",
    "\n",
    "    # Layers\n",
    "    num_anchors = 9\n",
    "    self._rpn_conv1 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = (3, 3), stride = 1, padding = \"same\")\n",
    "    self._rpn_class = nn.Conv2d(in_channels = 512, out_channels = num_anchors, kernel_size = (1, 1), stride = 1, padding = \"same\")\n",
    "    self._rpn_boxes = nn.Conv2d(in_channels = 512, out_channels = num_anchors * 4, kernel_size = (1, 1), stride = 1, padding = \"same\")\n",
    "    \n",
    "    # Initialize weights\n",
    "    self._rpn_conv1.weight.data.normal_(mean = 0.0, std = 0.01)\n",
    "    self._rpn_conv1.bias.data.zero_()\n",
    "    self._rpn_class.weight.data.normal_(mean = 0.0, std = 0.01)\n",
    "    self._rpn_class.bias.data.zero_()\n",
    "    self._rpn_boxes.weight.data.normal_(mean = 0.0, std = 0.01)\n",
    "    self._rpn_boxes.bias.data.zero_()\n",
    "\n",
    "  def forward(self, feature_map, image_shape, anchor_map, anchor_valid_map, max_proposals_pre_nms, max_proposals_post_nms):\n",
    "    \"\"\"\n",
    "    Predict objectness scores and regress region-of-interest box proposals on\n",
    "    an input feature map.\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_map : torch.Tensor\n",
    "      Feature map of shape (batch_size, 512, height, width).\n",
    "    image_shape : Tuple[int, int, int]\n",
    "      Shapes of each image in pixels: (num_channels, height, width).\n",
    "    anchor_map : np.ndarray\n",
    "      Map of anchors, shaped (height, width, num_anchors * 4). The last\n",
    "      dimension contains the anchor boxes specified as a 4-tuple of\n",
    "      (center_y, center_x, height, width), repeated for all anchors at that\n",
    "      coordinate of the feature map.\n",
    "    anchor_valid_map : np.ndarray\n",
    "      Map indicating which anchors are valid (do not intersect image bounds),\n",
    "      shaped (height, width, num_anchors).\n",
    "    max_proposals_pre_nms : int\n",
    "      How many of the best proposals (sorted by objectness score) to extract\n",
    "      before applying non-maximum suppression.\n",
    "    max_proposals_post_nms : int\n",
    "      How many of the best proposals (sorted by objectness score) to keep after\n",
    "      non-maximum suppression.\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor, torch.Tensor, torch.Tensor\n",
    "      - Objectness scores (batch_size, height, width, num_anchors)\n",
    "      - Box regressions (batch_size, height, width, num_anchors * 4), as box\n",
    "        deltas (that is, (ty, tx, th, tw) for each anchor)\n",
    "      - Proposals (N, 4) -- all corresponding proposal box corners stored as\n",
    "        (y1, x1, y2, x2).\n",
    "    \"\"\"\n",
    "\n",
    "    # Pass through the network\n",
    "    y = F.relu(self._rpn_conv1(feature_map))\n",
    "    objectness_score_map = t.sigmoid(self._rpn_class(y))\n",
    "    box_deltas_map = self._rpn_boxes(y)\n",
    "\n",
    "    # Transpose shapes to be more convenient:\n",
    "    #   objectness_score_map -> (batch_size, height, width, num_anchors)\n",
    "    #   box_deltas_map       -> (batch_size, height, width, num_anchors * 4)\n",
    "    objectness_score_map = objectness_score_map.permute(0, 2, 3, 1).contiguous()\n",
    "    box_deltas_map = box_deltas_map.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "    # Extract box deltas and anchors as (N,4) tensors and scores as (N,) list\n",
    "    anchors, objectness_scores, box_deltas = self._extract_valid(\n",
    "      anchor_map = anchor_map,\n",
    "      anchor_valid_map = anchor_valid_map,\n",
    "      objectness_score_map = objectness_score_map,\n",
    "      box_deltas_map = box_deltas_map\n",
    "    )\n",
    "\n",
    "    # Detach from graph to avoid backprop. According to my understanding, this\n",
    "    # should be redundant here because we later take care to detach the\n",
    "    # proposals (in FasterRCNNModel). However, there is a memory leak involving\n",
    "    # t_convert_deltas_to_boxes() if this is not done here. Ultimately, the\n",
    "    # numerical results are not affected. Proposals returned from this function\n",
    "    # are supposed to be constant and are fed into the detector stage. See any\n",
    "    # commit prior to 209141c for an earlier version of the code here that\n",
    "    # performed all operations on CPU using NumPy, which was slightly slower\n",
    "    # but equivalent.\n",
    "    box_deltas = box_deltas.detach()\n",
    "\n",
    "    # Convert regressions to box corners\n",
    "    proposals = t_convert_deltas_to_boxes(\n",
    "      box_deltas = box_deltas,\n",
    "      anchors = t.from_numpy(anchors).cuda(),\n",
    "      box_delta_means = t.tensor([0, 0, 0, 0], dtype = t.float32, device = \"cuda\"),\n",
    "      box_delta_stds = t.tensor([1, 1, 1, 1], dtype = t.float32, device = \"cuda\")\n",
    "    )\n",
    "\n",
    "    # Keep only the top-N scores. Note that we do not care whether the\n",
    "    # proposals were labeled as objects (score > 0.5) and peform a simple\n",
    "    # ranking among all of them. Restricting them has a strong adverse impact\n",
    "    # on training performance.\n",
    "    sorted_indices = t.argsort(objectness_scores)                   # sort in ascending order of objectness score\n",
    "    sorted_indices = sorted_indices.flip(dims = (0,))               # descending order of score\n",
    "    proposals = proposals[sorted_indices][0:max_proposals_pre_nms]  # grab the top-N best proposals\n",
    "    objectness_scores = objectness_scores[sorted_indices][0:max_proposals_pre_nms]  # corresponding scores\n",
    "\n",
    "    # Clip to image boundaries\n",
    "    proposals[:,0:2] = t.clamp(proposals[:,0:2], min = 0)\n",
    "    proposals[:,2] = t.clamp(proposals[:,2], max = image_shape[1])\n",
    "    proposals[:,3] = t.clamp(proposals[:,3], max = image_shape[2])\n",
    "\n",
    "    # Remove anything less than 16 pixels on a side\n",
    "    height = proposals[:,2] - proposals[:,0]\n",
    "    width = proposals[:,3] - proposals[:,1]\n",
    "    idxs = t.where((height >= 16) & (width >= 16))[0]\n",
    "    proposals = proposals[idxs]\n",
    "    objectness_scores = objectness_scores[idxs]\n",
    "\n",
    "    # Perform NMS\n",
    "    idxs = nms(\n",
    "      boxes = proposals,\n",
    "      scores = objectness_scores,\n",
    "      iou_threshold = 0.7\n",
    "    )\n",
    "    idxs = idxs[0:max_proposals_post_nms]\n",
    "    proposals = proposals[idxs]\n",
    "\n",
    "    # Return network outputs as PyTorch tensors and extracted object proposals\n",
    "    return objectness_score_map, box_deltas_map, proposals\n",
    "\n",
    "  def _extract_valid(self, anchor_map, anchor_valid_map, objectness_score_map, box_deltas_map):\n",
    "    assert objectness_score_map.shape[0] == 1 # only batch size of 1 supported for now\n",
    "\n",
    "    height, width, num_anchors = anchor_valid_map.shape\n",
    "    anchors = anchor_map.reshape((height * width * num_anchors, 4))             # [N,4] all anchors\n",
    "    anchors_valid = anchor_valid_map.reshape((height * width * num_anchors))    # [N,] whether anchors are valid (i.e., do not cross image boundaries)\n",
    "    scores = objectness_score_map.reshape((height * width * num_anchors))       # [N,] prediced objectness scores\n",
    "    box_deltas = box_deltas_map.reshape((height * width * num_anchors, 4))      # [N,4] predicted box delta regression targets\n",
    "\n",
    "    if self._allow_edge_proposals:\n",
    "      # Use all proposals\n",
    "      return anchors, scores, box_deltas\n",
    "    else:\n",
    "      # Filter out those proposals generated at invalid anchors\n",
    "      idxs = anchors_valid > 0\n",
    "      return anchors[idxs], scores[idxs], box_deltas[idxs]\n",
    "\n",
    "\n",
    "def class_loss(predicted_scores, y_true):\n",
    "  \"\"\"\n",
    "  Computes RPN class loss.\n",
    "  Parameters\n",
    "  ----------\n",
    "  predicted_scores : torch.Tensor\n",
    "    A tensor of shape (batch_size, height, width, num_anchors) containing\n",
    "    objectness scores (0 = background, 1 = object).\n",
    "  y_true : torch.Tensor\n",
    "    Ground truth tensor of shape (batch_size, height, width, num_anchors, 6).\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor\n",
    "    Scalar loss.\n",
    "  \"\"\"\n",
    "\n",
    "  epsilon = 1e-7\n",
    "\n",
    "  # y_true_class: (batch_size, height, width, num_anchors), same as predicted_scores\n",
    "  y_true_class = y_true[:,:,:,:,1].reshape(predicted_scores.shape)\n",
    "  y_predicted_class = predicted_scores\n",
    "  \n",
    "  # y_mask: y_true[:,:,:,0] is 1.0 for anchors included in the mini-batch\n",
    "  y_mask = y_true[:,:,:,:,0].reshape(predicted_scores.shape)\n",
    "\n",
    "  # Compute how many anchors are actually used in the mini-batch (e.g.,\n",
    "  # typically 256)\n",
    "  N_cls = t.count_nonzero(y_mask) + epsilon\n",
    "\n",
    "  # Compute element-wise loss for all anchors\n",
    "  loss_all_anchors = F.binary_cross_entropy(input = y_predicted_class, target = y_true_class, reduction = \"none\")\n",
    "  \n",
    "  # Zero out the ones which should not have been included\n",
    "  relevant_loss_terms = y_mask * loss_all_anchors\n",
    "\n",
    "  # Sum the total loss and normalize by the number of anchors used\n",
    "  return t.sum(relevant_loss_terms) / N_cls\n",
    "\n",
    "def regression_loss(predicted_box_deltas, y_true):\n",
    "  \"\"\"\n",
    "  Computes RPN box delta regression loss.\n",
    "  Parameters\n",
    "  ----------\n",
    "  predicted_box_deltas : torch.Tensor\n",
    "    A tensor of shape (batch_size, height, width, num_anchors * 4) containing\n",
    "    RoI box delta regressions for each anchor, stored as: ty, tx, th, tw.\n",
    "  y_true : torch.Tensor\n",
    "    Ground truth tensor of shape (batch_size, height, width, num_anchors, 6).\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor\n",
    "    Scalar loss.\n",
    "  \"\"\"\n",
    "  epsilon = 1e-7\n",
    "  scale_factor = 1.0  # hyper-parameter that controls magnitude of regression loss and is chosen to make regression term comparable to class term\n",
    "  sigma = 3.0         # see: https://github.com/rbgirshick/py-faster-rcnn/issues/89\n",
    "  sigma_squared = sigma * sigma\n",
    "\n",
    "  y_predicted_regression = predicted_box_deltas\n",
    "  y_true_regression = y_true[:,:,:,:,2:6].reshape(y_predicted_regression.shape)\n",
    "\n",
    "  # Include only anchors that are used in the mini-batch and which correspond\n",
    "  # to objects (positive samples)\n",
    "  y_included = y_true[:,:,:,:,0].reshape(y_true.shape[0:4]) # trainable anchors map: (batch_size, height, width, num_anchors)\n",
    "  y_positive = y_true[:,:,:,:,1].reshape(y_true.shape[0:4]) # positive anchors\n",
    "  y_mask = y_included * y_positive\n",
    "\n",
    "  # y_mask is of the wrong shape. We have one value per (y,x,k) position but in\n",
    "  # fact need to have 4 values (one for each of the regression variables). For\n",
    "  # example, y_predicted might be (1,37,50,36) and y_mask will be (1,37,50,9).\n",
    "  # We need to repeat the last dimension 4 times.\n",
    "  y_mask = y_mask.repeat_interleave(repeats = 4, dim = 3)\n",
    "\n",
    "  # The paper normalizes by dividing by a quantity called N_reg, which is equal\n",
    "  # to the total number of anchors (~2400) and then multiplying by lambda=10.\n",
    "  # This does not make sense to me because we are summing over a mini-batch at\n",
    "  # most, so we use N_cls here. I might be misunderstanding what is going on\n",
    "  # but 10/2400 = 1/240 which is pretty close to 1/256 and the paper mentions\n",
    "  # that training is relatively insensitve to choice of normalization.\n",
    "  N_cls = t.count_nonzero(y_included) + epsilon\n",
    "\n",
    "  # Compute element-wise loss using robust L1 function for all 4 regression\n",
    "  # components\n",
    "  x = y_true_regression - y_predicted_regression\n",
    "  x_abs = t.abs(x)\n",
    "  is_negative_branch = (x_abs < (1.0 / sigma_squared)).float()\n",
    "  R_negative_branch = 0.5 * x * x * sigma_squared\n",
    "  R_positive_branch = x_abs - 0.5 / sigma_squared\n",
    "  loss_all_anchors = is_negative_branch * R_negative_branch + (1.0 - is_negative_branch) * R_positive_branch\n",
    "\n",
    "  # Zero out the ones which should not have been included\n",
    "  relevant_loss_terms = y_mask * loss_all_anchors\n",
    "  return scale_factor * t.sum(relevant_loss_terms) / N_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_grad(func):\n",
    "  def wrapper_nograd(*args, **kwargs):\n",
    "    with t.no_grad():\n",
    "      return func(*args, **kwargs)\n",
    "  return wrapper_nograd\n",
    "\n",
    "class CSVLog:\n",
    "  \"\"\"\n",
    "  Logs to a CSV file.\n",
    "  \"\"\"\n",
    "  def __init__(self, filename):\n",
    "    self._filename = filename\n",
    "    self._header_written = False\n",
    "\n",
    "  def log(self, items):\n",
    "    keys = list(items.keys())\n",
    "    file_mode = \"a\" if self._header_written else \"w\"\n",
    "    with open(self._filename, file_mode) as fp:\n",
    "      if not self._header_written:\n",
    "        fp.write(\",\".join(keys) + \"\\n\")\n",
    "        self._header_written = True\n",
    "      values = [ str(value) for (key, value) in items.items() ]\n",
    "      fp.write(\",\".join(values) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNNModel(nn.Module):\n",
    "  @dataclass\n",
    "  class Loss:\n",
    "    rpn_class:            float\n",
    "    rpn_regression:       float\n",
    "    detector_class:       float\n",
    "    detector_regression:  float\n",
    "    total:                float\n",
    "\n",
    "  def __init__(self, num_classes, rpn_minibatch_size = 256, proposal_batch_size = 128, allow_edge_proposals = True, dropout_probability = 0):\n",
    "    super().__init__()\n",
    "\n",
    "    # Constants\n",
    "    self._num_classes = num_classes\n",
    "    self._rpn_minibatch_size = rpn_minibatch_size\n",
    "    self._proposal_batch_size = proposal_batch_size\n",
    "    self._detector_box_delta_means = [ 0, 0, 0, 0 ]\n",
    "    self._detector_box_delta_stds = [ 0.1, 0.1, 0.2, 0.2 ]\n",
    "\n",
    "    # Network stages\n",
    "    self._stage1_feature_extractor = FeatureExtractor()\n",
    "    self._stage2_region_proposal_network = RegionProposalNetwork(allow_edge_proposals = allow_edge_proposals)\n",
    "    self._stage3_detector_network = DetectorNetwork(num_classes = num_classes, dropout_probability = dropout_probability)\n",
    "\n",
    "  def forward(self, image_data, anchor_map = None, anchor_valid_map = None):\n",
    "    \"\"\"\n",
    "    Forward inference. Use for test and evaluation only.\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_data : torch.Tensor\n",
    "      A tensor of shape (batch_size, channels, height, width) representing\n",
    "      images normalized using the VGG-16 convention (BGR, ImageNet channel-wise\n",
    "      mean-centered).\n",
    "    anchor_map : torch.Tensor\n",
    "      Map of anchors, shaped (height, width, num_anchors * 4). The last\n",
    "      dimension contains the anchor boxes specified as a 4-tuple of\n",
    "      (center_y, center_x, height, width), repeated for all anchors at that\n",
    "      coordinate of the feature map. If this or anchor_valid_map is not\n",
    "      provided, both will be computed here.\n",
    "    anchor_valid_map : torch.Tensor\n",
    "      Map indicating which anchors are valid (do not intersect image bounds),\n",
    "      shaped (height, width). If this or anchor_map is not provided, both will\n",
    "      be computed here.\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, torch.Tensor, torch.Tensor\n",
    "      - Proposals (N, 4) from region proposal network\n",
    "      - Classes (M, num_classes) from detector network\n",
    "      - Box delta regressions (M, (num_classes - 1) * 4) from detector network\n",
    "    \"\"\"\n",
    "    assert image_data.shape[0] == 1, \"Batch size must be 1\"\n",
    "    image_shape = image_data.shape[1:]  # (batch_index, channels, height, width) -> (channels, height, width)\n",
    "\n",
    "    # Anchor maps can be pre-computed and passed in explicitly (for performance\n",
    "    # reasons) but if they are missing, we compute them on-the-fly here\n",
    "    if anchor_map is None or anchor_valid_map is None:\n",
    "      anchor_map, anchor_valid_map = generate_anchor_maps(image_shape = image_shape, feature_pixels = 16) \n",
    "\n",
    "    # Run each stage\n",
    "    feature_map = self._stage1_feature_extractor(image_data = image_data)\n",
    "    objectness_score_map, box_deltas_map, proposals = self._stage2_region_proposal_network(\n",
    "      feature_map = feature_map,\n",
    "      image_shape = image_shape,\n",
    "      anchor_map = anchor_map,\n",
    "      anchor_valid_map = anchor_valid_map,\n",
    "      max_proposals_pre_nms = 6000, # test time values\n",
    "      max_proposals_post_nms = 300\n",
    "    )\n",
    "    classes, box_deltas = self._stage3_detector_network(\n",
    "      feature_map = feature_map,\n",
    "      proposals = proposals\n",
    "    )\n",
    "\n",
    "    return proposals, classes, box_deltas\n",
    "\n",
    "  @no_grad\n",
    "  def predict(self, image_data, score_threshold, anchor_map = None, anchor_valid_map = None):\n",
    "    \"\"\"\n",
    "    Performs inference on an image and obtains the final detected boxes.\n",
    "    Parameters\n",
    "    ---------- \n",
    "    image_data : torch.Tensor\n",
    "      A tensor of shape (batch_size, channels, height, width) representing\n",
    "      images normalized using the VGG-16 convention (BGR, ImageNet channel-wise\n",
    "      mean-centered).\n",
    "    score_threshold : float\n",
    "      Minimum required score threshold (applied per class) for a detection to\n",
    "      be considered. Set this higher for visualization to minimize extraneous\n",
    "      boxes.\n",
    "    anchor_map : torch.Tensor\n",
    "      Map of anchors, shaped (height, width, num_anchors * 4). The last\n",
    "      dimension contains the anchor boxes specified as a 4-tuple of\n",
    "      (center_y, center_x, height, width), repeated for all anchors at that\n",
    "      coordinate of the feature map. If this or anchor_valid_map is not\n",
    "      provided, both will be computed here.\n",
    "    anchor_valid_map : torch.Tensor\n",
    "      Map indicating which anchors are valid (do not intersect image bounds),\n",
    "      shaped (height, width). If this or anchor_map is not provided, both will\n",
    "      be computed here.\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[int, np.ndarray]\n",
    "      Scored boxes, (N, 5) tensor of box corners and class score,\n",
    "      (y1, x1, y2, x2, score), indexed by class index.\n",
    "    \"\"\"\n",
    "    self.eval()\n",
    "    assert image_data.shape[0] == 1, \"Batch size must be 1\"\n",
    "\n",
    "    # Forward inference\n",
    "    proposals, classes, box_deltas = self(\n",
    "      image_data = image_data,\n",
    "      anchor_map = anchor_map,\n",
    "      anchor_valid_map = anchor_valid_map\n",
    "    )\n",
    "    proposals = proposals.cpu().numpy() \n",
    "    classes = classes.cpu().numpy()\n",
    "    box_deltas = box_deltas.cpu().numpy()\n",
    " \n",
    "    # Convert proposal boxes -> center point and size\n",
    "    proposal_anchors = np.empty(proposals.shape)\n",
    "    proposal_anchors[:,0] = 0.5 * (proposals[:,0] + proposals[:,2]) # center_y\n",
    "    proposal_anchors[:,1] = 0.5 * (proposals[:,1] + proposals[:,3]) # center_x\n",
    "    proposal_anchors[:,2:4] = proposals[:,2:4] - proposals[:,0:2]   # height, width\n",
    "\n",
    "    # Separate out results per class: class_idx -> (y1, x1, y2, x2, score)\n",
    "    boxes_and_scores_by_class_idx = {}\n",
    "    for class_idx in range(1, classes.shape[1]):  # skip class 0 (background)\n",
    "      # Get the box deltas (ty, tx, th, tw) corresponding to this class, for\n",
    "      # all proposals\n",
    "      box_delta_idx = (class_idx - 1) * 4\n",
    "      box_delta_params = box_deltas[:, (box_delta_idx + 0) : (box_delta_idx + 4)] # (N, 4)\n",
    "      proposal_boxes_this_class = convert_deltas_to_boxes(\n",
    "        box_deltas = box_delta_params,\n",
    "        anchors = proposal_anchors,\n",
    "        box_delta_means = self._detector_box_delta_means,\n",
    "        box_delta_stds = self._detector_box_delta_stds\n",
    "      )\n",
    "\n",
    "      # Clip to image boundaries\n",
    "      proposal_boxes_this_class[:,0::2] = np.clip(proposal_boxes_this_class[:,0::2], 0, image_data.shape[2] - 1)  # clip y1 and y2 to [0,height)\n",
    "      proposal_boxes_this_class[:,1::2] = np.clip(proposal_boxes_this_class[:,1::2], 0, image_data.shape[3] - 1)  # clip x1 and x2 to [0,width)\n",
    "\n",
    "      # Get the scores for this class. The class scores are returned in\n",
    "      # normalized categorical form. Each row corresponds to a class.\n",
    "      scores_this_class = classes[:,class_idx]\n",
    "\n",
    "      # Keep only those scoring high enough\n",
    "      sufficiently_scoring_idxs = np.where(scores_this_class > score_threshold)[0]\n",
    "      proposal_boxes_this_class = proposal_boxes_this_class[sufficiently_scoring_idxs]\n",
    "      scores_this_class = scores_this_class[sufficiently_scoring_idxs]\n",
    "      boxes_and_scores_by_class_idx[class_idx] = (proposal_boxes_this_class, scores_this_class)\n",
    "\n",
    "    # Perform NMS per class\n",
    "    scored_boxes_by_class_idx = {}\n",
    "    for class_idx, (boxes, scores) in boxes_and_scores_by_class_idx.items():\n",
    "      idxs = nms(\n",
    "        boxes = t.from_numpy(boxes).cuda(),\n",
    "        scores = t.from_numpy(scores).cuda(),\n",
    "        iou_threshold = 0.3 #TODO: unsure about this. Paper seems to imply 0.5 but https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/fast_rcnn/config.py has 0.3 for test NMS \n",
    "      ).cpu().numpy()\n",
    "      boxes = boxes[idxs]\n",
    "      scores = np.expand_dims(scores[idxs], axis = 0) # (N,) -> (N,1)\n",
    "      scored_boxes = np.hstack([ boxes, scores.T ])   # (N,5), with each row: (y1, x1, y2, x2, score)\n",
    "      scored_boxes_by_class_idx[class_idx] = scored_boxes\n",
    "\n",
    "    return scored_boxes_by_class_idx\n",
    "\n",
    "  def train_step(self, optimizer, image_data, anchor_map, anchor_valid_map, gt_rpn_map, gt_rpn_object_indices, gt_rpn_background_indices, gt_boxes):\n",
    "    \"\"\"\n",
    "    Performs one training step on a sample of data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : torch.optim.Optimizer\n",
    "      Optimizer.\n",
    "    image_data : torch.Tensor\n",
    "      A tensor of shape (batch_size, channels, height, width) representing\n",
    "      images normalized using the VGG-16 convention (BGR, ImageNet channel-wise\n",
    "      mean-centered).\n",
    "    anchor_map : torch.Tensor\n",
    "      Map of anchors, shaped (height, width, num_anchors * 4). The last\n",
    "      dimension contains the anchor boxes specified as a 4-tuple of\n",
    "      (center_y, center_x, height, width), repeated for all anchors at that\n",
    "      coordinate of the feature map. If this or anchor_valid_map is not\n",
    "      provided, both will be computed here.\n",
    "    anchor_valid_map : torch.Tensor\n",
    "      Map indicating which anchors are valid (do not intersect image bounds),\n",
    "      shaped (height, width). If this or anchor_map is not provided, both will\n",
    "      be computed here.\n",
    "    gt_rpn_map : torch.Tensor\n",
    "      Ground truth RPN map of shape\n",
    "      (batch_size, height, width, num_anchors, 6), where height and width are\n",
    "      the feature map dimensions, not the input image dimensions. The final\n",
    "      dimension contains:\n",
    "       - 0: Trainable anchor (1) or not (0). Only valid and non-neutral (that\n",
    "            is, definitely positive or negative) anchors are trainable. This is\n",
    "            the same as anchor_valid_map with additional invalid anchors caused\n",
    "            by neutral samples\n",
    "       - 1: For trainable anchors, whether the anchor is an object anchor (1)\n",
    "            or background anchor (0). For non-trainable anchors, will be 0.\n",
    "       - 2: Regression target for box center, ty.\n",
    "       - 3: Regression target for box center, tx.\n",
    "       - 4: Regression target for box size, th.\n",
    "       - 5: Regression target for box size, tw.\n",
    "    gt_rpn_object_indices : List[np.ndarray]\n",
    "      For each image in the batch, a map of shape (N, 3) of indices (y, x, k)\n",
    "      of all N object anchors in the RPN ground truth map.\n",
    "    gt_rpn_background_indices : List[np.ndarray]\n",
    "      For each image in the batch, a map of shape (M, 3) of indices of all M\n",
    "      background anchors in the RPN ground truth map.\n",
    "    gt_boxes : List[List[datasets.training_sample.Box]]\n",
    "      For each image in the batch, a list of ground truth object boxes.\n",
    "    Returns\n",
    "    -------\n",
    "    Loss \n",
    "      Loss (a dataclass with class and regression losses for both the RPN and\n",
    "      detector states).\n",
    "    \"\"\"\n",
    "    self.train()\n",
    "\n",
    "    # Clear accumulated gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # For now, we only support a batch size of 1\n",
    "    assert image_data.shape[0] == 1, \"Batch size must be 1\"\n",
    "    assert len(gt_rpn_map.shape) == 5 and gt_rpn_map.shape[0] == 1, \"Batch size must be 1\"\n",
    "    assert len(gt_rpn_object_indices) == 1, \"Batch size must be 1\"\n",
    "    assert len(gt_rpn_background_indices) == 1, \"Batch size must be 1\"\n",
    "    assert len(gt_boxes) == 1, \"Batch size must be 1\"\n",
    "    image_shape = image_data.shape[1:]\n",
    "\n",
    "    # Stage 1: Extract features\n",
    "    feature_map = self._stage1_feature_extractor(image_data = image_data)\n",
    "\n",
    "    # Stage 2: Generate object proposals using RPN \n",
    "    rpn_score_map, rpn_box_deltas_map, proposals = self._stage2_region_proposal_network(\n",
    "      feature_map = feature_map,\n",
    "      image_shape = image_shape,  # each image in batch has identical shape: (num_channels, height, width)\n",
    "      anchor_map = anchor_map,\n",
    "      anchor_valid_map = anchor_valid_map,\n",
    "      max_proposals_pre_nms = 12000,\n",
    "      max_proposals_post_nms = 2000\n",
    "    )\n",
    "\n",
    "    # Sample random mini-batch of anchors (for RPN training)\n",
    "    gt_rpn_minibatch_map = self._sample_rpn_minibatch(\n",
    "      rpn_map = gt_rpn_map,\n",
    "      object_indices = gt_rpn_object_indices,\n",
    "      background_indices = gt_rpn_background_indices\n",
    "    )\n",
    "\n",
    "    # Assign labels to proposals and take random sample (for detector training)\n",
    "    proposals, gt_classes, gt_box_deltas = self._label_proposals(\n",
    "      proposals = proposals,\n",
    "      gt_boxes = gt_boxes[0], # for now, batch size of 1\n",
    "      min_background_iou_threshold = 0.0,\n",
    "      min_object_iou_threshold = 0.5\n",
    "    )\n",
    "    proposals, gt_classes, gt_box_deltas = self._sample_proposals(\n",
    "      proposals = proposals,\n",
    "      gt_classes = gt_classes,\n",
    "      gt_box_deltas = gt_box_deltas,\n",
    "      max_proposals = self._proposal_batch_size,\n",
    "      positive_fraction = 0.25\n",
    "    )\n",
    "\n",
    "    # Make sure RoI proposals and ground truths are detached from computational\n",
    "    # graph so that gradients are not propagated through them. They are treated\n",
    "    # as constant inputs into the detector stage.\n",
    "    proposals = proposals.detach()\n",
    "    gt_classes = gt_classes.detach()\n",
    "    gt_box_deltas = gt_box_deltas.detach()\n",
    "\n",
    "    # Stage 3: Detector\n",
    "    detector_classes, detector_box_deltas = self._stage3_detector_network(\n",
    "      feature_map = feature_map,\n",
    "      proposals = proposals \n",
    "    )\n",
    "\n",
    "    # Compute losses\n",
    "    rpn_class_loss = class_loss(predicted_scores = rpn_score_map, y_true = gt_rpn_minibatch_map)\n",
    "    rpn_regression_loss = regression_loss(predicted_box_deltas = rpn_box_deltas_map, y_true = gt_rpn_minibatch_map)\n",
    "    detector_class_loss = class_loss(predicted_classes = detector_classes, y_true = gt_classes)\n",
    "    detector_regression_loss = regression_loss(predicted_box_deltas = detector_box_deltas, y_true = gt_box_deltas)\n",
    "    total_loss = rpn_class_loss + rpn_regression_loss + detector_class_loss + detector_regression_loss\n",
    "    loss = FasterRCNNModel.Loss(\n",
    "      rpn_class = rpn_class_loss.detach().cpu().item(),\n",
    "      rpn_regression = rpn_regression_loss.detach().cpu().item(),\n",
    "      detector_class = detector_class_loss.detach().cpu().item(),\n",
    "      detector_regression = detector_regression_loss.detach().cpu().item(),\n",
    "      total = total_loss.detach().cpu().item()\n",
    "    )\n",
    "\n",
    "    # Backprop\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return losses and data useful for computing statistics\n",
    "    return loss\n",
    "  \n",
    "  def _sample_rpn_minibatch(self, rpn_map, object_indices, background_indices):\n",
    "    \"\"\"\n",
    "    Selects anchors for training and produces a copy of the RPN ground truth\n",
    "    map with only those anchors marked as trainable.\n",
    "    Parameters\n",
    "    ----------\n",
    "    rpn_map : np.ndarray\n",
    "      RPN ground truth map of shape\n",
    "      (batch_size, height, width, num_anchors, 6).\n",
    "    object_indices : List[np.ndarray]\n",
    "      For each image in the batch, a map of shape (N, 3) of indices (y, x, k)\n",
    "      of all N object anchors in the RPN ground truth map.\n",
    "    background_indices : List[np.ndarray]\n",
    "      For each image in the batch, a map of shape (M, 3) of indices of all M\n",
    "      background anchors in the RPN ground truth map.\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "      A copy of the RPN ground truth map with index 0 of the last dimension\n",
    "      recomputed to include only anchors in the minibatch.\n",
    "    \"\"\"\n",
    "    assert rpn_map.shape[0] == 1, \"Batch size must be 1\"\n",
    "    assert len(object_indices) == 1, \"Batch size must be 1\"\n",
    "    assert len(background_indices) == 1, \"Batch size must be 1\"\n",
    "    positive_anchors = object_indices[0]\n",
    "    negative_anchors = background_indices[0]\n",
    "    assert len(positive_anchors) + len(negative_anchors) >= self._rpn_minibatch_size, \"Image has insufficient anchors for RPN minibatch size of %d\" % self._rpn_minibatch_size\n",
    "    assert len(positive_anchors) > 0, \"Image does not have any positive anchors\"\n",
    "    assert self._rpn_minibatch_size % 2 == 0, \"RPN minibatch size must be evenly divisible\"\n",
    "\n",
    "    # Sample, producing indices into the index maps\n",
    "    num_positive_anchors = len(positive_anchors)\n",
    "    num_negative_anchors = len(negative_anchors)\n",
    "    num_positive_samples = min(self._rpn_minibatch_size // 2, num_positive_anchors) # up to half the samples should be positive, if possible\n",
    "    num_negative_samples = self._rpn_minibatch_size - num_positive_samples          # the rest should be negative\n",
    "    positive_anchor_idxs = random.sample(range(num_positive_anchors), num_positive_samples)\n",
    "    negative_anchor_idxs = random.sample(range(num_negative_anchors), num_negative_samples)\n",
    "    \n",
    "    # Construct index expressions into RPN map\n",
    "    positive_anchors = positive_anchors[positive_anchor_idxs]\n",
    "    negative_anchors = negative_anchors[negative_anchor_idxs]\n",
    "    trainable_anchors = np.concatenate([ positive_anchors, negative_anchors ])\n",
    "    batch_idxs = np.zeros(len(trainable_anchors))\n",
    "    trainable_idxs = (batch_idxs, trainable_anchors[:,0], trainable_anchors[:,1], trainable_anchors[:,2], 0)\n",
    "\n",
    "    # Create a copy of the RPN map with samples set as trainable\n",
    "    rpn_minibatch_map = rpn_map.clone()\n",
    "    rpn_minibatch_map[:,:,:,:,0] = 0\n",
    "    rpn_minibatch_map[trainable_idxs] = 1\n",
    "\n",
    "    return rpn_minibatch_map\n",
    "\n",
    "  def _label_proposals(self, proposals, gt_boxes, min_background_iou_threshold, min_object_iou_threshold):\n",
    "    \"\"\"\n",
    "    Determines which proposals generated by the RPN stage overlap with ground\n",
    "    truth boxes and creates ground truth labels for the subsequent detector\n",
    "    stage.\n",
    "    Parameters\n",
    "    ----------\n",
    "    proposals : torch.Tensor\n",
    "      Proposal corners, shaped (N, 4).\n",
    "    gt_boxes : List[datasets.training_sample.Box]\n",
    "      Ground truth object boxes.\n",
    "    min_background_iou_threshold : float\n",
    "      Minimum IoU threshold with ground truth boxes below which proposals are\n",
    "      ignored entirely. Proposals with an IoU threshold in the range\n",
    "      [min_background_iou_threshold, min_object_iou_threshold) are labeled as\n",
    "      background. This value can be greater than 0, which has the effect of\n",
    "      selecting more difficult background examples that have some degree of\n",
    "      overlap with ground truth boxes.\n",
    "    min_object_iou_threshold : float\n",
    "      Minimum IoU threshold for a proposal to be labeled as an object.\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor, torch.Tensor, torch.Tensor\n",
    "      Proposals, (N, 4), labeled as either objects or background (depending on\n",
    "      IoU thresholds, some proposals can end up as neither and are excluded\n",
    "      here); one-hot encoded class labels, (N, num_classes), for each proposal;\n",
    "      and box delta regression targets, (N, 2, (num_classes - 1) * 4), for each\n",
    "      proposal. Box delta target values are present at locations [:,1,:] and\n",
    "      consist of (ty, tx, th, tw) for the class that the box corresponds to.\n",
    "      The entries for all other classes and the background classes should be\n",
    "      ignored. A mask is written to locations [:,0,:]. For each proposal\n",
    "      assigned a non-background class, there will be 4 consecutive elements\n",
    "      marked with 1 indicating the corresponding box delta target values are to\n",
    "      be used. There are no box delta regression targets for background\n",
    "      proposals and the mask is entirely 0 for those proposals.\n",
    "    \"\"\"\n",
    "    assert min_background_iou_threshold < min_object_iou_threshold, \"Object threshold must be greater than background threshold\"\n",
    "\n",
    "    # Convert ground truth box corners to (M,4) tensor and class indices to (M,)\n",
    "    gt_box_corners = np.array([ box.corners for box in gt_boxes ], dtype = np.float32)\n",
    "    gt_box_corners = t.from_numpy(gt_box_corners).cuda()\n",
    "    gt_box_class_idxs = t.tensor([ box.class_index for box in gt_boxes ], dtype = t.long, device = \"cuda\")\n",
    "\n",
    "    # Let's be crafty and create some fake proposals that match the ground\n",
    "    # truth boxes exactly. This isn't strictly necessary and the model should\n",
    "    # work without it but it will help training and will ensure that there are\n",
    "    # always some positive examples to train on. \n",
    "    proposals = t.vstack([ proposals, gt_box_corners ])\n",
    "\n",
    "    # Compute IoU between each proposal (N,4) and each ground truth box (M,4)\n",
    "    # -> (N, M)\n",
    "    ious = t_intersection_over_union(boxes1 = proposals, boxes2 = gt_box_corners)\n",
    "\n",
    "    # Find the best IoU for each proposal, the class of the ground truth box\n",
    "    # associated with it, and the box corners\n",
    "    best_ious = t.max(ious, dim = 1).values         # (N,) of maximum IoUs for each of the N proposals\n",
    "    box_idxs = t.argmax(ious, dim = 1)              # (N,) of ground truth box index for each proposal\n",
    "    gt_box_class_idxs = gt_box_class_idxs[box_idxs] # (N,) of class indices of highest-IoU box for each proposal\n",
    "    gt_box_corners = gt_box_corners[box_idxs]       # (N,4) of box corners of highest-IoU box for each proposal\n",
    " \n",
    "    # Remove all proposals whose best IoU is less than the minimum threshold\n",
    "    # for a negative (background) sample. We also check for IoUs > 0 because\n",
    "    # due to earlier clipping, we may get invalid 0-area proposals.\n",
    "    idxs = t.where((best_ious >= min_background_iou_threshold))[0]  # keep proposals w/ sufficiently high IoU\n",
    "    proposals = proposals[idxs]\n",
    "    best_ious = best_ious[idxs]\n",
    "    gt_box_class_idxs = gt_box_class_idxs[idxs]\n",
    "    gt_box_corners = gt_box_corners[idxs]\n",
    "\n",
    "    # IoUs less than min_object_iou_threshold will be labeled as background\n",
    "    gt_box_class_idxs[best_ious < min_object_iou_threshold] = 0\n",
    "    \n",
    "    # One-hot encode class labels\n",
    "    num_proposals = proposals.shape[0]\n",
    "    gt_classes = t.zeros((num_proposals, self._num_classes), dtype = t.float32, device = \"cuda\")  # (N,num_classes)\n",
    "    gt_classes[ t.arange(num_proposals), gt_box_class_idxs ] = 1.0\n",
    "\n",
    "    # Convert proposals and ground truth boxes into \"anchor\" format (center\n",
    "    # points and side lengths). For the detector stage, the proposals serve as\n",
    "    # the anchors relative to which the final box predictions will be \n",
    "    # regressed.\n",
    "    proposal_centers = 0.5 * (proposals[:,0:2] + proposals[:,2:4])          # center_y, center_x\n",
    "    proposal_sides = proposals[:,2:4] - proposals[:,0:2]                    # height, width\n",
    "    gt_box_centers = 0.5 * (gt_box_corners[:,0:2] + gt_box_corners[:,2:4])  # center_y, center_x\n",
    "    gt_box_sides = gt_box_corners[:,2:4] - gt_box_corners[:,0:2]            # height, width\n",
    "\n",
    "    # Compute box delta regression targets (ty, tx, th, tw) for each proposal\n",
    "    # based on the best box selected\n",
    "    box_delta_targets = t.empty((num_proposals, 4), dtype = t.float32, device = \"cuda\") # (N,4)\n",
    "    box_delta_targets[:,0:2] = (gt_box_centers - proposal_centers) / proposal_sides # ty = (gt_center_y - proposal_center_y) / proposal_height, tx = (gt_center_x - proposal_center_x) / proposal_width\n",
    "    box_delta_targets[:,2:4] = t.log(gt_box_sides / proposal_sides)                 # th = log(gt_height / proposal_height), tw = (gt_width / proposal_width)\n",
    "    box_delta_means = t.tensor(self._detector_box_delta_means, dtype = t.float32, device = \"cuda\")\n",
    "    box_delta_stds = t.tensor(self._detector_box_delta_stds, dtype = t.float32, device = \"cuda\")\n",
    "    box_delta_targets[:,:] -= box_delta_means                               # mean adjustment\n",
    "    box_delta_targets[:,:] /= box_delta_stds                                # standard deviation scaling\n",
    "\n",
    "    # Convert regression targets into a map of shape (N,2,4*(C-1)) where C is\n",
    "    # the number of classes and [:,0,:] specifies a mask for the corresponding\n",
    "    # target components at [:,1,:]. Targets are ordered (ty, tx, th, tw).\n",
    "    # Background class 0 is not present at all.\n",
    "    gt_box_deltas = t.zeros((num_proposals, 2, 4 * (self._num_classes - 1)), dtype = t.float32, device = \"cuda\")\n",
    "    gt_box_deltas[:,0,:] = t.repeat_interleave(gt_classes, repeats = 4, dim = 1)[:,4:]  # create masks using interleaved repetition, remembering to ignore class 0\n",
    "    gt_box_deltas[:,1,:] = t.tile(box_delta_targets, dims = (1, self._num_classes - 1)) # populate regression targets with straightforward repetition (only those columns corresponding to class are masked on)\n",
    "\n",
    "    return proposals, gt_classes, gt_box_deltas\n",
    "\n",
    "  def _sample_proposals(self, proposals, gt_classes, gt_box_deltas, max_proposals, positive_fraction):\n",
    "    if max_proposals <= 0:\n",
    "      return proposals, gt_classes, gt_box_deltas\n",
    "  \n",
    "    # Get positive and negative (background) proposals\n",
    "    class_indices = t.argmax(gt_classes, axis = 1)  # (N,num_classes) -> (N,), where each element is the class index (highest score from its row)\n",
    "    positive_indices = t.where(class_indices > 0)[0]\n",
    "    negative_indices = t.where(class_indices <= 0)[0]\n",
    "    num_positive_proposals = len(positive_indices)\n",
    "    num_negative_proposals = len(negative_indices)\n",
    "    \n",
    "    # Select positive and negative samples, if there are enough. Note that the\n",
    "    # number of positive samples can be either the positive fraction of the\n",
    "    # *actual* number of proposals *or* the *desired* number (max_proposals).\n",
    "    # In practice, these yield virtually identical results but the latter\n",
    "    # method will yield slightly more positive samples in the rare cases when \n",
    "    # the number of proposals is below the desired number. Here, we use the\n",
    "    # former method but others, such as Yun Chen, use the latter. To implement\n",
    "    # it, replace num_samples with max_proposals in the line that computes\n",
    "    # num_positive_samples. I am not sure what the original Faster R-CNN\n",
    "    # implementation does.\n",
    "    num_samples = min(max_proposals, len(class_indices))\n",
    "    num_positive_samples = min(round(num_samples * positive_fraction), num_positive_proposals)\n",
    "    num_negative_samples = min(num_samples - num_positive_samples, num_negative_proposals)\n",
    "  \n",
    "    # Do we have enough?\n",
    "    if num_positive_samples <= 0 or num_negative_samples <= 0:\n",
    "      return proposals[[]], gt_classes[[]], gt_box_deltas[[]] # return 0-length tensors\n",
    "  \n",
    "    # Sample randomly\n",
    "    positive_sample_indices = positive_indices[ t.randperm(len(positive_indices))[0:num_positive_samples] ]\n",
    "    negative_sample_indices = negative_indices[ t.randperm(len(negative_indices))[0:num_negative_samples] ]\n",
    "    indices = t.cat([ positive_sample_indices, negative_sample_indices ])\n",
    "  \n",
    "    # Return\n",
    "    return proposals[indices], gt_classes[indices], gt_box_deltas[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingStatistics:\n",
    "  \"\"\"\n",
    "  Computes statistics per epoch.\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    self.rpn_class_loss = float(\"inf\")\n",
    "    self.rpn_regression_loss = float(\"inf\")\n",
    "    self.detector_class_loss = float(\"inf\")\n",
    "    self.detector_regression_loss = float(\"inf\")\n",
    "    self._rpn_class_losses = []\n",
    "    self._rpn_regression_losses = []\n",
    "    self._detector_class_losses = []\n",
    "    self._detector_regression_losses = []\n",
    "\n",
    "  def on_training_step(self, loss):\n",
    "    \"\"\"\n",
    "    Call once per training iteration to aggregate losses.\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss : models.faster_rcnn.FasterRCNNModel.Loss\n",
    "      Dataclass containing losses broken down by RPN and detector, and further\n",
    "      by classifier and regression loss. \n",
    "    \"\"\"\n",
    "    self._rpn_class_losses.append(loss.rpn_class)\n",
    "    self._rpn_regression_losses.append(loss.rpn_regression)\n",
    "    self._detector_class_losses.append(loss.detector_class)\n",
    "    self._detector_regression_losses.append(loss.detector_regression)\n",
    "    self.rpn_class_loss = np.mean(self._rpn_class_losses)\n",
    "    self.rpn_regression_loss = np.mean(self._rpn_regression_losses)\n",
    "    self.detector_class_loss = np.mean(self._detector_class_losses)\n",
    "    self.detector_regression_loss = np.mean(self._detector_regression_losses)\n",
    "\n",
    "  def get_progbar_postfix(self):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "      A dictionary of labels and values suitable for use as a postfix object\n",
    "      for a tqdm progress bar.\n",
    "    \"\"\"\n",
    "    return { \n",
    "      \"rpn_class_loss\": \"%1.4f\" % self.rpn_class_loss,\n",
    "      \"rpn_regr_loss\": \"%1.4f\" % self.rpn_regression_loss,\n",
    "      \"detector_class_loss\": \"%1.4f\" % self.detector_class_loss,\n",
    "      \"detector_regr_loss\": \"%1.4f\" % self.detector_regression_loss,\n",
    "      \"total_loss\": \"%1.2f\" % (self.rpn_class_loss + self.rpn_regression_loss + self.detector_class_loss + self.detector_regression_loss)\n",
    "    }\n",
    "\n",
    "\n",
    "class PrecisionRecallCurveCalculator:\n",
    "  \"\"\"\n",
    "  Collects data over the course of a validation pass and then computes\n",
    "  precision and recall (including mean average precision).\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    # List of (confidence_score, correctness) by class for all images in dataset\n",
    "    self._unsorted_predictions_by_class_index = defaultdict(list)\n",
    "\n",
    "    # True number of objects by class for all images in dataset\n",
    "    self._object_count_by_class_index = defaultdict(int)\n",
    "\n",
    "  def _compute_correctness_of_predictions(self, scored_boxes_by_class_index, gt_boxes):\n",
    "    unsorted_predictions_by_class_index = {}\n",
    "    object_count_by_class_index = defaultdict(int)\n",
    "\n",
    "    # Count objects by class. We do this here because in case there are no\n",
    "    # predictions, we do not want to miscount the total number of objects.\n",
    "    for gt_box in gt_boxes:\n",
    "      object_count_by_class_index[gt_box.class_index] += 1\n",
    "\n",
    "    for class_index, scored_boxes in scored_boxes_by_class_index.items():\n",
    "      # Get the ground truth boxes corresponding to this class\n",
    "      gt_boxes_this_class = [ gt_box for gt_box in gt_boxes if gt_box.class_index == class_index ]\n",
    "\n",
    "      # Compute IoU of each box with each ground truth box and store as a list\n",
    "      # of tuples (iou, box_index, gt_box_index) by descending IoU\n",
    "      ious = []\n",
    "      for gt_idx in range(len(gt_boxes_this_class)):\n",
    "        for box_idx in range(len(scored_boxes)):\n",
    "          boxes1 = np.expand_dims(scored_boxes[box_idx][0:4], axis = 0) # convert single box (4,) to (1,4), as expected by parallel IoU function\n",
    "          boxes2 = np.expand_dims(gt_boxes_this_class[gt_idx].corners, axis = 0)\n",
    "          iou = intersection_over_union(boxes1 = boxes1, boxes2 = boxes2) \n",
    "          ious.append((iou, box_idx, gt_idx))\n",
    "      ious = sorted(ious, key = lambda iou: ious[0], reverse = True)  # sort descending by IoU\n",
    "      \n",
    "      # Vector that indicates whether a ground truth box has been detected\n",
    "      gt_box_detected = [ False ] * len(gt_boxes)\n",
    "\n",
    "      # Vector that indicates whether a prediction is a true positive (True) or\n",
    "      # false positive (False)\n",
    "      is_true_positive = [ False ] * len(scored_boxes)\n",
    "      \n",
    "      #\n",
    "      # Construct a list of prediction descriptions: (score, correct)\n",
    "      # Score is the confidence score of the predicted box and correct is\n",
    "      # whether it is a true positive (True) or false positive (False).\n",
    "      #\n",
    "      # A true positive is a prediction that has an IoU of > 0.5 and is\n",
    "      # also the highest-IoU prediction for a ground truth box. Predictions\n",
    "      # with IoU <= 0.5 or that do not have the highest IoU for any ground\n",
    "      # truth box are considered false positives.\n",
    "      #\n",
    "      iou_threshold = 0.5\n",
    "      for iou, box_idx, gt_idx in ious:\n",
    "        if iou <= iou_threshold:\n",
    "          continue\n",
    "        if is_true_positive[box_idx] or gt_box_detected[gt_idx]:\n",
    "          # The prediction and/or ground truth box have already been matched\n",
    "          continue\n",
    "        # We've got a true positive\n",
    "        is_true_positive[box_idx] = True\n",
    "        gt_box_detected[gt_idx] = True\n",
    "      # Construct the final array of prediction descriptions\n",
    "      unsorted_predictions_by_class_index[class_index] = [ (scored_boxes[i][4], is_true_positive[i]) for i in range(len(scored_boxes)) ]\n",
    "        \n",
    "    return unsorted_predictions_by_class_index, object_count_by_class_index\n",
    "\n",
    "  def add_image_results(self, scored_boxes_by_class_index, gt_boxes):\n",
    "    \"\"\"\n",
    "    Adds a detection result to the running tally. Should be called only once per\n",
    "    image in the dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    scored_boxes_by_class_index : dict\n",
    "      Final detected boxes as lists of tuples, (y_min, x_min, y_max, x_max,\n",
    "      score), by class index. The score is the softmax output and is\n",
    "      interpreted as a confidence metric when sorting results for the mAP\n",
    "      calculation.\n",
    "    gt_boxes : list\n",
    "      A list of datasets.training_sample.Box objects describing all ground\n",
    "      truth boxes in the image.\n",
    "    \"\"\"\n",
    "    # Merge in results for this single image\n",
    "    unsorted_predictions_by_class_index, object_count_by_class_index = self._compute_correctness_of_predictions(\n",
    "      scored_boxes_by_class_index = scored_boxes_by_class_index,\n",
    "      gt_boxes = gt_boxes) \n",
    "    for class_index, predictions in unsorted_predictions_by_class_index.items():\n",
    "      self._unsorted_predictions_by_class_index[class_index] += predictions\n",
    "    for class_index, count in object_count_by_class_index.items():\n",
    "      self._object_count_by_class_index[class_index] += object_count_by_class_index[class_index]\n",
    "\n",
    "  def _compute_average_precision(self, class_index):\n",
    "    # Sort predictions in descending order of score\n",
    "    sorted_predictions = sorted(self._unsorted_predictions_by_class_index[class_index], key = lambda prediction: prediction[0], reverse = True)\n",
    "    num_ground_truth_positives = self._object_count_by_class_index[class_index]\n",
    "\n",
    "    # Compute raw recall and precision arrays\n",
    "    recall_array = []\n",
    "    precision_array = []\n",
    "    true_positives = 0  # running tally\n",
    "    false_positives = 0 # \"\"\n",
    "    for i in range(len(sorted_predictions)):\n",
    "      true_positives += 1 if sorted_predictions[i][1] == True else 0\n",
    "      false_positives += 0 if sorted_predictions[i][1] == True else 1\n",
    "      recall = true_positives / num_ground_truth_positives\n",
    "      precision = true_positives / (true_positives + false_positives)\n",
    "      recall_array.append(recall)\n",
    "      precision_array.append(precision)\n",
    "\n",
    "    # Insert 0 at the beginning and end of the list. The 0 at the beginning won't\n",
    "    # matter due to how interpolation works, below.\n",
    "    recall_array.insert(0, 0.0)\n",
    "    recall_array.append(1.0)\n",
    "    precision_array.insert(0, 0.0)\n",
    "    precision_array.append(0.0)\n",
    "\n",
    "    # Interpolation means we compute the highest precision observed at a given\n",
    "    # recall value. Specifically, it means taking the maximum value seen from\n",
    "    # each point onward. See URL below:\n",
    "    # https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52#1a59\n",
    "    for i in range(len(precision_array)):\n",
    "      precision_array[i] = np.max(precision_array[i:])\n",
    "    \n",
    "    # Compute AP using simple rectangular integration under the curve\n",
    "    average_precision = 0\n",
    "    for i in range(len(recall_array) - 1):\n",
    "      dx = recall_array[i + 1] - recall_array[i + 0]\n",
    "      dy = precision_array[i + 1]\n",
    "      average_precision += dy * dx\n",
    "\n",
    "    return average_precision, recall_array, precision_array\n",
    "\n",
    "  def compute_mean_average_precision(self):\n",
    "    \"\"\"\n",
    "    Calculates mAP (mean average precision) using all the data accumulated thus\n",
    "    far. This should be called only after all image results have been\n",
    "    processed.\n",
    "    Returns\n",
    "    -------\n",
    "    np.float64\n",
    "      Mean average precision.\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    for class_index in self._object_count_by_class_index:\n",
    "      average_precision, _, _ = self._compute_average_precision(class_index = class_index)\n",
    "      average_precisions.append(average_precision)\n",
    "    return np.mean(average_precisions)\n",
    "  \n",
    "  def plot_precision_vs_recall(self, class_index, class_name = None, interpolated = False):\n",
    "    \"\"\"\n",
    "    Plots precision (y axis) vs. recall (x axis) using all the data accumulated\n",
    "    thus far. This should be called only after all image results have been\n",
    "    processed.\n",
    "    Parameters\n",
    "    ----------\n",
    "    class_index : int\n",
    "      The class index for which the curve is plotted.\n",
    "    class_name : str\n",
    "      If given, used as the class name on the plot label. Otherwise, the\n",
    "      numeric class index is used directly.\n",
    "    \"\"\"\n",
    "    average_precision, recall_array, precision_array = self._compute_average_precision(class_index = class_index, interpolated = interpolated)\n",
    "\n",
    "    # Plot raw precision vs. recall\n",
    "    import matplotlib.pyplot as plt\n",
    "    label = \"{0} AP={1:1.2f}\".format(\"Class {}\".format(class_index) if class_name is None else class_name, average_precision)\n",
    "    plt.plot(recall_array, precision_array, label = label)\n",
    "    if interpolated:\n",
    "      plt.title(\"Precision (Interpolated) vs. Recall\")\n",
    "    else:\n",
    "      plt.title(\"Precision vs. Recall\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "  def plot_average_precisions(self, class_index_to_name): \n",
    "    # Compute average precisions for each class\n",
    "    labels = [ class_index_to_name[class_index] for class_index in self._object_count_by_class_index ]\n",
    "    average_precisions = []\n",
    "    for class_index in self._object_count_by_class_index:\n",
    "      average_precision, _, _ = self._compute_average_precision(class_index = class_index)\n",
    "      average_precisions.append(average_precision)\n",
    "\n",
    "    # Sort alphabetically by class name\n",
    "    sorted_results = sorted(zip(labels, average_precisions), reverse = True, key = lambda pair: pair[0])\n",
    "    labels, average_precisions = zip(*sorted_results) # unzip \n",
    "    \n",
    "    # Convert to %\n",
    "    average_precisions = np.array(average_precisions) * 100.0 # convert to %\n",
    "\n",
    "    # Bar plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.clf()\n",
    "    plt.xlim([0, 100])\n",
    "    plt.barh(labels, average_precisions)\n",
    "    plt.title(\"Model Performance\")\n",
    "    plt.xlabel(\"Average Precision (%)\")\n",
    "    for index, value in enumerate(average_precisions):\n",
    "      plt.text(value, index, \"%1.1f\" % value)\n",
    "    plt.show()\n",
    "\n",
    "  def print_average_precisions(self, class_index_to_name):\n",
    "    # Compute average precisions for each class\n",
    "    labels = [ class_index_to_name[class_index] for class_index in self._object_count_by_class_index ]\n",
    "    average_precisions = []\n",
    "    for class_index in self._object_count_by_class_index:\n",
    "      average_precision, _, _ = self._compute_average_precision(class_index = class_index)\n",
    "      average_precisions.append(average_precision)\n",
    "\n",
    "    # Sort by score (descending)\n",
    "    sorted_results = sorted(zip(labels, average_precisions), reverse = True, key = lambda pair: pair[1])\n",
    "    _, average_precisions = zip(*sorted_results) # unzip\n",
    "\n",
    "    # Maximum width of any class name (for pretty printing)\n",
    "    label_width = max([ len(label) for label in labels ])\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"Average Precisions\")\n",
    "    print(\"------------------\")\n",
    "    for (label, average_precision) in sorted_results:\n",
    "      print(\"%s: %1.1f%%\" % (label.ljust(label_width), average_precision * 100.0))\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_rectangle(ctx, corners, color, thickness = 4):\n",
    "  y_min, x_min, y_max, x_max = corners\n",
    "  ctx.rectangle(xy = [(x_min, y_min), (x_max, y_max)], outline = color, width = thickness)\n",
    "\n",
    "def _draw_text(image, text, position, color, scale = 1.0, offset_lines = 0):\n",
    "  \"\"\"\n",
    "  Parameters\n",
    "  ----------\n",
    "  image : PIL.Image\n",
    "    Image object to draw on.\n",
    "  text : str\n",
    "    Text to render.\n",
    "  position : Tuple[float, float]\n",
    "    Location of top-left corner of text string in pixels.\n",
    "  offset_lines : float\n",
    "    Number of lines to offset the vertical position by, where a line is the\n",
    "    text height.\n",
    "  \"\"\"\n",
    "  font = ImageFont.load_default()\n",
    "  text_size = font.getsize(text)\n",
    "  text_image = Image.new(mode = \"RGBA\", size = text_size, color = (0, 0, 0, 0))\n",
    "  ctx = ImageDraw.Draw(text_image)\n",
    "  ctx.text(xy = (0, 0), text = text, font = font, fill = color)\n",
    "  scaled = text_image.resize((round(text_image.width * scale), round(text_image.height * scale)))\n",
    "  position = (round(position[0]), round(position[1] + offset_lines * scaled.height))\n",
    "  image.paste(im = scaled, box = position, mask = scaled)\n",
    "\n",
    "def _class_to_color(class_index):\n",
    "  return list(ImageColor.colormap.values())[class_index + 1]\n",
    "\n",
    "def show_anchors(output_path, image, anchor_map, anchor_valid_map, gt_rpn_map, gt_boxes, display = False):\n",
    "  ctx = ImageDraw.Draw(image, mode = \"RGBA\")\n",
    "  \n",
    "  # Draw all ground truth boxes with thick green lines\n",
    "  for box in gt_boxes:\n",
    "    _draw_rectangle(ctx, corners = box.corners, color = (0, 255, 0))\n",
    "\n",
    "  # Draw all object anchor boxes in yellow\n",
    "  for y in range(anchor_valid_map.shape[0]):\n",
    "    for x in range(anchor_valid_map.shape[1]):\n",
    "      for k in range(anchor_valid_map.shape[2]):  \n",
    "        if anchor_valid_map[y,x,k] <= 0 or gt_rpn_map[y,x,k,0] <= 0:\n",
    "          continue  # skip anchors excluded from training\n",
    "        if gt_rpn_map[y,x,k,1] < 1:\n",
    "          continue  # skip background anchors\n",
    "        height = anchor_map[y,x,k*4+2]\n",
    "        width = anchor_map[y,x,k*4+3]\n",
    "        cy = anchor_map[y,x,k*4+0]\n",
    "        cx = anchor_map[y,x,k*4+1]\n",
    "        corners = (cy - 0.5 * height, cx - 0.5 * width, cy + 0.5 * height, cx + 0.5 * width)\n",
    "        _draw_rectangle(ctx, corners = corners, color = (255, 255, 0), thickness = 3)\n",
    " \n",
    "  image.save(output_path)\n",
    "  if display:\n",
    "    image.show()\n",
    "\n",
    "def show_detections(output_path, show_image, image, scored_boxes_by_class_index, class_index_to_name):\n",
    "  # Draw all results\n",
    "  ctx = ImageDraw.Draw(image, mode = \"RGBA\")\n",
    "  color_idx = 0\n",
    "  for class_index, scored_boxes in scored_boxes_by_class_index.items():\n",
    "    for i in range(scored_boxes.shape[0]):\n",
    "      scored_box = scored_boxes[i,:]\n",
    "      class_name = class_index_to_name[class_index]\n",
    "      text = \"%s %1.2f\" % (class_name, scored_box[4])\n",
    "      color = _class_to_color(class_index = class_index)\n",
    "      _draw_rectangle(ctx = ctx, corners = scored_box[0:4], color = color, thickness = 2)\n",
    "      _draw_text(image = image, text = text, position = (scored_box[1], scored_box[0]), color = color, scale = 1.5, offset_lines = -1)\n",
    "\n",
    "  # Output\n",
    "  if show_image:\n",
    "    image.show()\n",
    "  if output_path is not None:\n",
    "    image.save(output_path)\n",
    "    print(\"Wrote detection results to '%s'\" % output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_keras_weights(hdf5_file, layer_name):\n",
    "  \"\"\"\n",
    "  Loads Keras-formatted weights from an HDF5 file and returns them as a PyTorch\n",
    "  tensor.\n",
    "  Parameters\n",
    "  ----------\n",
    "  hdf5_file : h5py.File\n",
    "    Opened HDF5 file object.\n",
    "  layer_name : str\n",
    "    Name of layer to load. E.g., \"fc1\".\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor\n",
    "    Weights or None if layer not found.\n",
    "  \"\"\"\n",
    "  primary_keypath = \"model_weights/\" + layer_name\n",
    "  for keypath, node in hdf5_file[primary_keypath].items():\n",
    "    if keypath.startswith(\"conv\") or keypath.startswith(\"dense\"):\n",
    "      kernel_keypath = \"/\".join([primary_keypath, keypath, \"kernel:0\"])\n",
    "      weights = np.array(hdf5_file[kernel_keypath]).astype(np.float32)\n",
    "      return t.from_numpy(weights).cuda()\n",
    "  return None\n",
    "\n",
    "def _load_keras_biases(hdf5_file, layer_name):\n",
    "  \"\"\"\n",
    "  Loads Keras-formatted biases from an HDF5 file and returns them as a PyTorch\n",
    "  vector.\n",
    "  Parameters\n",
    "  ----------\n",
    "  hdf5_file : h5py.File\n",
    "    Opened HDF5 file object.\n",
    "  layer_name : str\n",
    "    Name of the layer to load. E.g., \"block1_conv1\".\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor\n",
    "    Bias vector or None if layer not be found.\n",
    "  \"\"\"\n",
    "  primary_keypath = \"model_weights/\" + layer_name\n",
    "  for keypath, node in hdf5_file[primary_keypath].items():\n",
    "    if keypath.startswith(\"conv\") or keypath.startswith(\"dense\"):\n",
    "      bias_keypath = \"/\".join([primary_keypath, keypath, \"bias:0\"])\n",
    "      biases = np.array(hdf5_file[bias_keypath]).astype(np.float32)\n",
    "      return t.from_numpy(biases).cuda()\n",
    "  return None\n",
    "\n",
    "def _load_keras_layer(hdf5_file, layer_name):\n",
    "  \"\"\"\n",
    "  Loads Keras-formatted weights and biases from an HDF5 file and returns them\n",
    "  as PyTorch tensors.\n",
    "  Parameters\n",
    "  ----------\n",
    "  hdf5_file : h5py.File\n",
    "    Opened HDF5 file object.\n",
    "  layer_name : str\n",
    "    Name of layer to load. E.g., \"fc1\".\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor, torch.Tensor\n",
    "    Weights and biases. One or both can be None if not found.\n",
    "  \"\"\"\n",
    "  return _load_keras_weights(hdf5_file = hdf5_file, layer_name = layer_name), _load_keras_biases(hdf5_file = hdf5_file, layer_name = layer_name)\n",
    "\n",
    "def _load_keras_conv2d_layer(hdf5_file, layer_name, keras_shape = None):\n",
    "  \"\"\"\n",
    "  Loads Keras-formatted 2D convolutional kernel weights and biases from an HDF5\n",
    "  file and returns them as PyTorch tensors. Keras stores kernels as:\n",
    "    (kernel_height, kernel_width, channels_in, channels_out)\n",
    "  \n",
    "  PyTorch:\n",
    "    (channels_out, channels_in, kernel_height, kernel_width)\n",
    "  Parameters\n",
    "  ----------\n",
    "  hdf5_file : h5py.File\n",
    "    Opened HDF5 file object.\n",
    "  layer_name : str\n",
    "    Name of layer to load. E.g., \"block1_conv1\".\n",
    "  keras_shape : tuple\n",
    "    Original Keras shape. If specified, weights are reshaped to this shape\n",
    "    before being transposed to PyTorch format.\n",
    "  Returns\n",
    "  -------\n",
    "  torch.Tensor, torch.Tensor\n",
    "    Weights and biases. One or both can be None if not found.\n",
    "  \"\"\"\n",
    "  weights, biases = _load_keras_layer(hdf5_file = hdf5_file, layer_name = layer_name)\n",
    "  if weights is not None and biases is not None:\n",
    "    if keras_shape is not None:\n",
    "      weights = weights.reshape(keras_shape)\n",
    "    weights = weights.permute([ 3, 2, 0, 1 ])\n",
    "  return weights, biases\n",
    "\n",
    "def _load_vgg16_from_bart_keras_model(filepath):\n",
    "  missing_layers = []\n",
    "  state = {}\n",
    "  file = h5py.File(filepath, \"r\")\n",
    "\n",
    "  # Feature extractor\n",
    "  keras_layers = [\n",
    "    \"block1_conv1\",\n",
    "    \"block1_conv2\",\n",
    "    \"block2_conv1\",\n",
    "    \"block2_conv2\",\n",
    "    \"block3_conv1\",\n",
    "    \"block3_conv2\",\n",
    "    \"block3_conv3\",\n",
    "    \"block4_conv1\",\n",
    "    \"block4_conv2\",\n",
    "    \"block4_conv3\",\n",
    "    \"block5_conv1\",\n",
    "    \"block5_conv2\",\n",
    "    \"block5_conv3\"\n",
    "  ]\n",
    "  for layer_name in keras_layers:\n",
    "    weights, biases = _load_keras_conv2d_layer(hdf5_file = file, layer_name = layer_name)\n",
    "    if weights is not None and biases is not None:\n",
    "      state[\"_stage1_feature_extractor._\" + layer_name + \".weight\"] = weights \n",
    "      state[\"_stage1_feature_extractor._\" + layer_name + \".bias\"] = biases\n",
    "    else:\n",
    "      missing_layers.append(layer_name)\n",
    "\n",
    "  # Detector\n",
    "  weights, biases = _load_keras_layer(hdf5_file = file, layer_name = \"fc1\")\n",
    "  if weights is not None and biases is not None:\n",
    "    # The fc1 layer in Keras takes as input a flattened (7, 7, 512) map from\n",
    "    # the RoI pool layer. Here in PyTorch, it is (512, 7, 7). Keras stores\n",
    "    # weights as (25088, 4096), which is equivalent to (7, 7, 512, 4096), as\n",
    "    # per Keras channels-last convention. To convert to PyTorch, we must\n",
    "    # first transpose to (512, 7, 7, 4096), then flatten to (25088, 4096),\n",
    "    # and, lastly, transpose to (4096, 25088).\n",
    "    weights = weights.reshape((7, 7, 512, 4096))\n",
    "    weights = weights.permute([ 2, 0, 1, 3 ]) # (512, 7, 7, 4096)\n",
    "    weights = weights.reshape((-1, 4096))     # (25088, 4096)\n",
    "    weights = weights.permute([ 1, 0 ])       # (4096, 25088)\n",
    "    state[\"_stage3_detector_network._fc1.weight\"] = weights\n",
    "    state[\"_stage3_detector_network._fc1.bias\"] = biases\n",
    "  else:\n",
    "    missing_layers.append(\"fc1\") \n",
    "  weights, biases = _load_keras_layer(hdf5_file = file, layer_name = \"fc2\")\n",
    "  if weights is not None and biases is not None:\n",
    "    # Due to the adjustment for fc1, fc2 can be loaded with only a transpose\n",
    "    # of the two components (in_dimension, out_dimension) -> \n",
    "    # (out_dimension, in_dimension).\n",
    "    state[\"_stage3_detector_network._fc2.weight\"] = weights.permute([ 1, 0 ])\n",
    "    state[\"_stage3_detector_network._fc2.bias\"] = biases\n",
    "  else:\n",
    "    missing_layers.append(\"fc2\")\n",
    "\n",
    "  # Anything missing?\n",
    "  if len(missing_layers) > 0:    \n",
    "    print(\"Some layers were missing from '%s' and not loaded: %s\" % (filepath, \", \".join(missing_layers)))\n",
    "\n",
    "  return state\n",
    "\n",
    "def _load_vgg16_from_caffe_model(filepath):\n",
    "  state = {}\n",
    "  caffe = t.load(filepath)\n",
    "\n",
    "  # Attempt to load all layers\n",
    "  mapping = {\n",
    "    \"features.0.\":    \"_stage1_feature_extractor._block1_conv1\",\n",
    "    \"features.2.\":    \"_stage1_feature_extractor._block1_conv2\",\n",
    "    \"features.5.\":    \"_stage1_feature_extractor._block2_conv1\",\n",
    "    \"features.7.\":    \"_stage1_feature_extractor._block2_conv2\",\n",
    "    \"features.10.\":   \"_stage1_feature_extractor._block3_conv1\",\n",
    "    \"features.12.\":   \"_stage1_feature_extractor._block3_conv2\",\n",
    "    \"features.14.\":   \"_stage1_feature_extractor._block3_conv3\",\n",
    "    \"features.17.\":   \"_stage1_feature_extractor._block4_conv1\",\n",
    "    \"features.19.\":   \"_stage1_feature_extractor._block4_conv2\",\n",
    "    \"features.21.\":   \"_stage1_feature_extractor._block4_conv3\",\n",
    "    \"features.24.\":   \"_stage1_feature_extractor._block5_conv1\",\n",
    "    \"features.26.\":   \"_stage1_feature_extractor._block5_conv2\",\n",
    "    \"features.28.\":   \"_stage1_feature_extractor._block5_conv3\",\n",
    "    \"classifier.0.\":  \"_stage3_detector_network._fc1\",\n",
    "    \"classifier.3.\":  \"_stage3_detector_network._fc2\"\n",
    "  }\n",
    "  missing_layers = set([ layer_name[:-1] for layer_name in mapping.keys() ])  # we will remove as we load\n",
    "  for key, tensor in caffe.items():\n",
    "    caffe_layer_name = \".\".join(key.split(\".\")[0:2])  # grab first two parts\n",
    "    caffe_key = caffe_layer_name + \".\"                # add trailing '.' for key in mapping dict\n",
    "    if caffe_key in mapping:\n",
    "      weight_key = caffe_key + \"weight\"\n",
    "      bias_key = caffe_key + \"bias\"\n",
    "      if weight_key in caffe and bias_key in caffe:\n",
    "        state[mapping[caffe_key] + \".weight\"] = caffe[weight_key]\n",
    "        state[mapping[caffe_key] + \".bias\"] = caffe[bias_key]\n",
    "        missing_layers.discard(caffe_layer_name)\n",
    "\n",
    "  # If *all* were missing, this file must not contain the Caffe VGG-16 model\n",
    "  if len(missing_layers) == len(mapping):\n",
    "    raise ValueError(\"File '%s' is not a Caffe VGG-16 model\" % filepath)\n",
    "\n",
    "  if len(missing_layers) > 0:\n",
    "    print(\"Some layers were missing from '%s' and not loaded: %s\" % (filepath, \", \".join(missing_layers)))\n",
    "    \n",
    "  return state\n",
    "\n",
    "def load(model, filepath):\n",
    "  \"\"\"\n",
    "  Load model wieghts and biases from a file. We support 3 different formats:\n",
    "  \n",
    "    - PyTorch state files containing our complete model as-is\n",
    "    - PyTorch state files containing only VGG-16 layers trained in Caffe (i.e.,\n",
    "      the published reference implementation of VGG-16). These are compatible\n",
    "      with the VGG-16 image normalization used here, unlike the torchvision\n",
    "      VGG-16 implementation. The Caffe state file can be found online and is\n",
    "      usually named vgg16_caffe.pth.\n",
    "    - Keras h5 state file containing only VGG-16 layers trained by my own\n",
    "      VGG-16 model (github.com/trzy/VGG16).\n",
    "  Parameters\n",
    "  ----------\n",
    "  model : torch.nn.Module\n",
    "    The complete Faster R-CNN model to load weights and biases into.\n",
    "  filepath : str\n",
    "    File to load.\n",
    "  \"\"\"\n",
    "\n",
    "  state = None\n",
    "\n",
    "  # Keras?\n",
    "  try:\n",
    "    state = _load_vgg16_from_bart_keras_model(filepath = filepath)\n",
    "    print(\"Loaded initial VGG-16 layer weights from Keras model '%s'\" % filepath)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  # Caffe?\n",
    "  if state is None:\n",
    "    try:\n",
    "      state = _load_vgg16_from_caffe_model(filepath = filepath)\n",
    "      print(\"Loaded initial VGG-16 layer weights from Caffe model '%s'\" % filepath)\n",
    "    except Exception as e:\n",
    "      pass\n",
    "\n",
    "  # Assume complete PyTorch state\n",
    "  if state is None:\n",
    "    state = t.load(filepath)\n",
    "    if \"model_state_dict\" not in state:\n",
    "      raise KeyError(\"Model state file '%s' is missing top-level key 'model_state_dict'\" % filepath)\n",
    "    state = state[\"model_state_dict\"]\n",
    "\n",
    "  # Load\n",
    "  try:\n",
    "    model.load_state_dict(state)\n",
    "    print(\"Loaded initial weights from '%s'\" % filepath)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return\n",
    "\n",
    "class BestWeightsTracker:\n",
    "  def __init__(self, filepath):\n",
    "    self._filepath = filepath\n",
    "    self._best_state = None\n",
    "    self._best_mAP = 0\n",
    "\n",
    "  def on_epoch_end(self, model, epoch, mAP):\n",
    "    if mAP > self._best_mAP:\n",
    "      self._best_mAP = mAP\n",
    "      self._best_state = { \"epoch\": epoch, \"model_state_dict\": model.state_dict() }\n",
    "\n",
    "  def save_best_weights(self, model):\n",
    "    if self._best_state is not None:\n",
    "      t.save(self._best_state, self._filepath)\n",
    "      print(\"Saved best model weights (Mean Average Precision = %1.2f%%) to '%s'\" % (self._best_mAP, self._filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.dataset_dir = \" \"\n",
    "        self.profile_cuda_memory = False\n",
    "        self.train_split = \"Train\"\n",
    "        self.eval_split = \"Test\"\n",
    "        self.dump_anchors = \"dump\"\n",
    "        self.train = True\n",
    "        self.epochs = 10\n",
    "        self.learning_rate = 1e-3\n",
    "        self.checkpoit_dit = \"checkpoint_dir\"\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 1e-5\n",
    "        self.dropout = 0\n",
    "        self.no_augment = True\n",
    "        self.exclude_edge_proposals = True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_anchors():\n",
    "  training_data = Dataset(dir = options.dataset_dir, split = options.train_split, augment = False, shuffle = False)\n",
    "  if not os.path.exists(options.dump_anchors):\n",
    "    os.makedirs(options.dump_anchors)\n",
    "  print(\"Rendering anchors from '%s' to set '%s'...\" % (options.train_split, options.dump_anchors))\n",
    "  for sample in iter(training_data):\n",
    "    output_path = os.path.join(options.dump_anchors, \"anchors_\" + os.path.basename(sample.filepath) + \".png\")\n",
    "    show_anchors(\n",
    "      output_path = output_path,\n",
    "      image = sample.image,\n",
    "      anchor_map = sample.anchor_map,\n",
    "      anchor_valid_map = sample.anchor_valid_map,\n",
    "      gt_rpn_map = sample.gt_rpn_map,\n",
    "      gt_boxes = sample.gt_boxes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_data = None, num_samples = None, plot = False, print_average_precisions = False):\n",
    "  if eval_data is None:\n",
    "    eval_data = Dataset(dir = options.dataset_dir, split = options.eval_split, augment = False, shuffle = False)\n",
    "  if num_samples is None:\n",
    "    num_samples = eval_data.num_samples\n",
    "  precision_recall_curve = PrecisionRecallCurveCalculator()\n",
    "  i = 0\n",
    "  print(\"Evaluating '%s'...\" % eval_data.split)\n",
    "  for sample in tqdm(iterable = iter(eval_data), total = num_samples):\n",
    "    scored_boxes_by_class_index = model.predict(\n",
    "      image_data = t.from_numpy(sample.image_data).unsqueeze(dim = 0).cuda(),\n",
    "      score_threshold = 0.05  # lower threshold for evaluation\n",
    "    )\n",
    "    precision_recall_curve.add_image_results(\n",
    "      scored_boxes_by_class_index = scored_boxes_by_class_index,\n",
    "      gt_boxes = sample.gt_boxes\n",
    "    )\n",
    "    i += 1\n",
    "    if i >= num_samples:\n",
    "      break\n",
    "  if print_average_precisions:\n",
    "    precision_recall_curve.print_average_precisions(class_index_to_name = Dataset.class_index_to_name)\n",
    "  mean_average_precision = 100.0 * precision_recall_curve.compute_mean_average_precision()\n",
    "  print(\"Mean Average Precision = %1.2f%%\" % mean_average_precision)\n",
    "  if plot:\n",
    "    precision_recall_curve.plot_average_precisions(class_index_to_name = Dataset.class_index_to_name)\n",
    "  return mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "  params = []\n",
    "  for key, value in dict(model.named_parameters()).items():\n",
    "    if not value.requires_grad:\n",
    "      continue\n",
    "    if \"weight\" in key:\n",
    "      params += [{ \"params\": [value], \"weight_decay\": options.weight_decay }]\n",
    "  return t.optim.SGD(params, lr = options.learning_rate, momentum = options.momentum)\n",
    "\n",
    "# def enable_cuda_memory_profiler(model):\n",
    "#   from . import profile\n",
    "#   import sys\n",
    "#   import threading\n",
    "#   memory_profiler = profile.CUDAMemoryProfiler([ model ], filename = \"cuda_memory.txt\")\n",
    "#   sys.settrace(memory_profiler)\n",
    "#   threading.settrace(memory_profiler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "  # if options.profile_cuda_memory:\n",
    "  #   enable_cuda_memory_profiler(model = model)\n",
    "  print(\"Training Parameters\")\n",
    "  print(\"-------------------\")\n",
    "\n",
    "  print(\"Dataset           : %s\" % options.dataset_dir)\n",
    "  print(\"Training split    : %s\" % options.train_split)\n",
    "  print(\"Evaluation split  : %s\" % options.eval_split)\n",
    "  print(\"Epochs            : %d\" % options.epochs)\n",
    "  print(\"Learning rate     : %f\" % options.learning_rate)\n",
    "  print(\"Momentum          : %f\" % options.momentum)\n",
    "  print(\"Weight decay      : %f\" % options.weight_decay)\n",
    "  print(\"Dropout           : %f\" % options.dropout)\n",
    "  print(\"Augmentation      : %s\" % (\"disabled\" if options.no_augment else \"enabled\"))\n",
    "  print(\"Edge proposals    : %s\" % (\"excluded\" if options.exclude_edge_proposals else \"included\"))\n",
    "  print(\"CSV log           : %s\" % (\"none\" if not options.log_csv else options.log_csv))\n",
    "  print(\"Checkpoints       : %s\" % (\"disabled\" if not options.checkpoint_dir else options.checkpoint_dir))\n",
    "  print(\"Final weights file: %s\" % (\"none\" if not options.save_to else options.save_to))\n",
    "  print(\"Best weights file : %s\" % (\"none\" if not options.save_best_to else options.save_best_to))\n",
    "  training_data = Dataset(dir = options.dataset_dir, split = \"Train\", augment = not options.no_augment, shuffle = True, cache = options.cache_images)\n",
    "  eval_data = Dataset(dir = options.dataset_dir, split = \"Test\", augment = False, shuffle = False, cache = False)\n",
    "  optimizer = create_optimizer(model = model)\n",
    "  if options.checkpoint_dir and not os.path.exists(options.checkpoint_dir):\n",
    "    os.makedirs(options.checkpoint_dir)\n",
    "  if options.log_csv:\n",
    "    csv = CSVLog(options.log_csv)\n",
    "  if options.save_best_to:\n",
    "    best_weights_tracker = BestWeightsTracker(filepath = options.save_best_to)\n",
    "  for epoch in range(1, 1 + options.epochs):\n",
    "    print(\"Epoch %d/%d\" % (epoch, options.epochs))\n",
    "    stats = TrainingStatistics()\n",
    "    progbar = tqdm(iterable = iter(training_data), total = training_data.num_samples, postfix = stats.get_progbar_postfix())\n",
    "    for sample in progbar:\n",
    "      loss = model.train_step(  # don't retain any tensors we don't need (helps memory usage)\n",
    "        optimizer = optimizer,\n",
    "        image_data = t.from_numpy(sample.image_data).unsqueeze(dim = 0).cuda(),\n",
    "        anchor_map = sample.anchor_map,\n",
    "        anchor_valid_map = sample.anchor_valid_map,\n",
    "        gt_rpn_map = t.from_numpy(sample.gt_rpn_map).unsqueeze(dim = 0).cuda(),\n",
    "        gt_rpn_object_indices = [ sample.gt_rpn_object_indices ],\n",
    "        gt_rpn_background_indices = [ sample.gt_rpn_background_indices ],\n",
    "        gt_boxes = [ sample.gt_boxes ]\n",
    "      )\n",
    "      stats.on_training_step(loss = loss)\n",
    "      progbar.set_postfix(stats.get_progbar_postfix())\n",
    "    last_epoch = epoch == options.epochs\n",
    "    mean_average_precision = evaluate(\n",
    "      model = model,\n",
    "      eval_data = eval_data,\n",
    "      num_samples = options.periodic_eval_samples,\n",
    "      plot = False,\n",
    "      print_average_precisions = False\n",
    "    )\n",
    "    if options.checkpoint_dir:\n",
    "      checkpoint_file = os.path.join(options.checkpoint_dir, \"checkpoint-epoch-%d-mAP-%1.1f.pth\" % (epoch, mean_average_precision))\n",
    "      t.save({ \"epoch\": epoch, \"model_state_dict\": model.state_dict() }, checkpoint_file)\n",
    "      print(\"Saved model checkpoint to '%s'\" % checkpoint_file)\n",
    "    if options.log_csv:\n",
    "      log_items = {\n",
    "        \"epoch\": epoch,\n",
    "        \"learning_rate\": options.learning_rate,\n",
    "        \"momentum\": options.momentum,\n",
    "        \"weight_decay\": options.weight_decay,\n",
    "        \"dropout\": options.dropout,\n",
    "        \"mAP\": mean_average_precision\n",
    "      }\n",
    "      log_items.update(stats.get_progbar_postfix())\n",
    "      csv.log(log_items)\n",
    "    if options.save_best_to:\n",
    "      best_weights_tracker.on_epoch_end(model = model, epoch = epoch, mAP = mean_average_precision)\n",
    "  if options.save_to:\n",
    "    t.save({ \"epoch\": epoch, \"model_state_dict\": model.state_dict() }, options.save_to)\n",
    "    print(\"Saved final model weights to '%s'\" % options.save_to)\n",
    "  if options.save_best_to:\n",
    "    best_weights_tracker.save_best_weights(model = model)\n",
    "  print(\"Evaluating %s model on all samples in '%s'...\" % ((\"best\" if options.save_best_to else \"final\"), options.eval_split))  # evaluate final or best model on full dataset\n",
    "  evaluate(\n",
    "    model = model,\n",
    "    eval_data = eval_data,\n",
    "    num_samples = eval_data.num_samples,  # use all samples\n",
    "    plot = options.plot,\n",
    "    print_average_precisions = True\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 4.00 GiB total capacity; 3.13 GiB already allocated; 0 bytes free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-534fe71492ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Construct model and load initial weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m model = FasterRCNNModel(\n\u001b[0m\u001b[0;32m      3\u001b[0m    \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m    \u001b[0mallow_edge_proposals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m    \u001b[0mdropout_probability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\emrek\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \"\"\"\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\emrek\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\emrek\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\emrek\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\emrek\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \"\"\"\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 4.00 GiB total capacity; 3.13 GiB already allocated; 0 bytes free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    " # Construct model and load initial weights\n",
    "model = FasterRCNNModel(\n",
    "    num_classes = Dataset.num_classes,\n",
    "    allow_edge_proposals = False,\n",
    "    dropout_probability = 0\n",
    ").cuda()\n",
    "\n",
    "\n",
    "train(model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_data, image, show_image, output_path):\n",
    "  image_data = t.from_numpy(image_data).unsqueeze(dim = 0).cuda()\n",
    "  scored_boxes_by_class_index = model.predict(image_data = image_data, score_threshold = 0.7)\n",
    "  show_detections(\n",
    "    output_path = output_path,\n",
    "    show_image = show_image,\n",
    "    image = image,\n",
    "    scored_boxes_by_class_index = scored_boxes_by_class_index,\n",
    "    class_index_to_name = Dataset.class_index_to_name\n",
    "  )\n",
    "\n",
    "def predict_one(model, url, show_image, output_path):\n",
    "  image_data, image, _, _ = load_image(url = url, min_dimension_pixels = 600)\n",
    "  predict(model = model, image_data = image_data, image = image, show_image = show_image, output_path = output_path)\n",
    "\n",
    "def predict_all(model, split):\n",
    "  dirname = \"predictions_\" + split\n",
    "  if not os.path.exists(dirname):\n",
    "    os.makedirs(dirname)\n",
    "  print(\"Rendering predictions from '%s' set to '%s'...\" % (split, dirname))\n",
    "  dataset = Dataset(dir = options.dataset_dir, split = split, augment = False, shuffle = False)\n",
    "  for sample in iter(dataset):\n",
    "    output_path = os.path.join(dirname, os.path.splitext(os.path.basename(sample.filepath))[0] + \".png\")\n",
    "    predict(model = model, image_data = sample.image_data, image = sample.image, show_image = False, output_path = output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "  # Perform mutually exclusive procedures\n",
    "  if options.train:\n",
    "    train(model = model)\n",
    "  elif options.eval:\n",
    "    evaluate(model = model, plot = options.plot, print_average_precisions = True)\n",
    "  elif options.predict:\n",
    "    predict_one(model = model, url = options.predict, show_image = True, output_path = None)\n",
    "  elif options.predict_to_file:\n",
    "    predict_one(model = model, url = options.predict_to_file, show_image = False, output_path = \"predictions.png\")\n",
    "  elif options.predict_all:\n",
    "    predict_all(model = model, split = options.predict_all)\n",
    "  elif not options.dump_anchors:\n",
    "    print(\"Nothing to do. Did you mean to use --train or --predict?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region Proposal Network Arcihtecture\n",
    "\n",
    "\n",
    "class RPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RPN, self).__init__()\n",
    "\n",
    "        # VGG\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # We are not using fully connected layers (3 fc layers) as we need feature maps as output from this network.\n",
    "\n",
    "\n",
    "        # VGG16 Architecture Used for Creating Feature Maps, Now the RPN Layer. Number of Anchors = 9\n",
    "\n",
    "        self.rpn_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.class_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=9, kernel_size=1),\n",
    "            nn.Sigmoid()#0 or 1 to get label is in anchorbox or not\n",
    "        )\n",
    "        \n",
    "        self.regr_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=36, kernel_size=1),\n",
    "            nn.Linear(in_channels=512, out_channels=36)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.layer1(x)\n",
    "            out = self.layer2(out)\n",
    "            out = self.layer3(out)\n",
    "            out = self.layer4(out)\n",
    "            out = self.layer5(out)\n",
    "\n",
    "            out = self.rpn_layer(out)\n",
    "\n",
    "            out_class = self.class_layer(out)\n",
    "\n",
    "            out_regr = self.regr_layer(out)\n",
    "\n",
    "            return [out_class, out_regr, out] #classification object(0,1) in the anchor boxes and bounding boxes\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(25088, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(self.conv3_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = F.relu(self.conv4_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv5_1(x))\n",
    "        x = F.relu(self.conv5_2(x))\n",
    "        x = F.relu(self.conv5_3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5) #dropout was included to combat overfitting\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "398b1f7ed72a7068b3f6e41c8d0b7c46f955388764fdd26a380830a749e28b27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
